\chapter{Implicit Functions and Extremum Problems}

\section{Jacobians}

\subsection*{Essential Definitions and Theorems}

\begin{definition}[Jacobian Determinant]
For a $C^1$ map $f = (f_1, \ldots, f_n): \mathbb{R}^n \to \mathbb{R}^n$, the Jacobian determinant at a point $x$ is:
\[J_f(x) = \det\left[\frac{\partial f_i}{\partial x_j}(x)\right] = \det[Df(x)]\]
where $Df(x)$ is the derivative matrix of $f$ at $x$.
\end{definition}

\noindent\textbf{Importance:} The Jacobian determinant measures how the transformation $f$ changes volumes and orientations locally. It's essential for change of variables in multiple integrals, understanding local invertibility, and coordinate transformations. The sign of the Jacobian indicates whether the transformation preserves or reverses orientation.



\begin{definition}[Chain Rule for Jacobians]
If $h = f \circ g$ where $f, g: \mathbb{R}^n \to \mathbb{R}^n$ are differentiable, then:
\[J_h(x) = J_f(g(x)) \cdot J_g(x)\]
\end{definition}

\noindent\textbf{Importance:} This rule allows us to compute Jacobians of complicated transformations by breaking them down into simpler ones. It's essential for understanding how multiple coordinate changes combine and for proving properties of composite transformations.



\begin{theorem}[Inverse Function Theorem]
If $f: \mathbb{R}^n \to \mathbb{R}^n$ is $C^1$ in a neighborhood of $x_0$ and $J_f(x_0) \neq 0$, then $f$ is locally invertible near $x_0$. The inverse function $f^{-1}$ is also $C^1$ and:
\[Df^{-1}(f(x_0)) = [Df(x_0)]^{-1}\]
\end{theorem}

\noindent\textbf{Importance:} This theorem provides conditions under which a function has a local inverse, which is crucial for solving systems of equations, coordinate transformations, and understanding the local behavior of differentiable maps. The condition $J_f(x_0) \neq 0$ is essential.



\begin{theorem}[Matrix Determinant Lemma]
For compatible vectors $u, v \in \mathbb{R}^n$:
\[\det(I + uv^T) = 1 + v^T u\]
\end{theorem}

\noindent\textbf{Importance:} This lemma provides an efficient way to compute determinants of matrices that differ from the identity by a rank-one matrix. It's particularly useful for computing Jacobians of certain types of transformations and for understanding how small perturbations affect determinants.




\begin{problembox}[13.1: Complex Function Jacobian]
\begin{problemstatement}
Let \( f \) be the complex-valued function defined for each complex \( z \neq 0 \) by the equation \( f(z) = 1/\bar{z} \). Show that \( J_f(z) = -|z|^{-4} \). Show that \( f \) is one-to-one and compute \( f^{-1} \) explicitly.
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Express the complex function in terms of real variables $(x,y)$, compute the partial derivatives to find the Jacobian determinant, then verify injectivity by solving for the inverse function.

\bigskip\noindent\textbf{Solution:}
Write $z=x+iy$, so $\bar z=x-iy$ and $f(z)=1/\bar z=\dfrac{x+iy}{x^2+y^2}$. Therefore
\[u(x,y)=\frac{x}{x^2+y^2},\quad v(x,y)=\frac{y}{x^2+y^2}.
\]
Compute
\[u_x=\frac{y^2-x^2}{(x^2+y^2)^2},\; u_y=\frac{-2xy}{(x^2+y^2)^2},\; v_x=\frac{-2xy}{(x^2+y^2)^2},\; v_y=\frac{x^2-y^2}{(x^2+y^2)^2}.
\]
Thus
\[J_f=u_xv_y-u_yv_x=-\frac{(x^2+y^2)^2}{(x^2+y^2)^4}=-|z|^{-4}.
\]
Moreover, $w=f(z)=1/\bar z$ implies $\bar w=1/z$, so $z=1/\bar w$. Hence $f$ is one-to-one with inverse $f^{-1}(w)=1/\bar w$.\qed


\begin{problembox}[13.2: Vector-Valued Function Jacobian]
\begin{problemstatement}
Let \( f = (f_1, f_2, f_3) \) be the vector-valued function defined (for every point \( (x_1, x_2, x_3) \) in \( R^3 \) for which \( x_1 + x_2 + x_3 \neq -1 \)) as follows:
\[f_k(x_1, x_2, x_3) = \frac{x_k}{1 + x_1 + x_2 + x_3} \quad (k = 1, 2, 3).\]
Show that \( J_f(x_1, x_2, x_3) = (1 + x_1 + x_2 + x_3)^{-4} \). Show that \( f \) is one-to-one and compute \( f^{-1} \) explicitly.
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Use the matrix determinant lemma to compute the Jacobian efficiently, then solve the system of equations to find the inverse function explicitly.

\bigskip\noindent\textbf{Solution:}
Let $S=x_1+x_2+x_3$ and $m=(1+S)^{-1}$. Then $f_i=x_im$ and
\[\frac{\partial f_i}{\partial x_j}=m\,\delta_{ij}-m^2 x_i=(mI-m^2xe^{\!T})_{ij},\]
where $x=(x_1,x_2,x_3)^{\!T}$ and $e=(1,1,1)^{\!T}$. Hence
\[\det Df=m^3\det\big(I-mxe^{\!T}\big)=m^3\big(1-me^{\!T}x\big)=m^4=(1+S)^{-4}.
\]
Solving $y_i=\dfrac{x_i}{1+S}$ gives $x_i=Ty_i$ with $T=1+S$. Summing yields $S=T\sum y_i$, so $T(1-\sum y_i)=1$ and $T=\dfrac{1}{1-\sum y_i}$. Therefore
\[f^{-1}(y)=\left(\frac{y_1}{1-\sum y_i},\frac{y_2}{1-\sum y_i},\frac{y_3}{1-\sum y_i}\right),\]
valid when $\sum y_i\neq 1$. Thus $f$ is one-to-one on its domain.\qed


\begin{problembox}[13.3: Composition of Functions Jacobian]
\begin{problemstatement}
Let \( f = (f_1, \ldots, f_n) \) be a vector-valued function defined in \( R^n \), suppose \( f \in C' \) on \( R^n \), and let \( J_f(x) \) denote the Jacobian determinant. Let \( g_1, \ldots, g_n \) be \( n \) real-valued functions defined on \( R^1 \) and having continuous derivatives \( g'_1, \ldots, g'_n \). Let \( h_k(x) = f_k[g_1(x_1), \ldots, g_n(x_n)], k = 1, 2, \ldots, n \), and put \( h = (h_1, \ldots, h_n) \). Show that
\[J_h(x) = J_f[g_1(x_1), \ldots, g_n(x_n)]g'_1(x_1) \cdots g'_n(x_n).\]
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Apply the chain rule for Jacobians to the composition $h=f\circ G$ where $G$ is a diagonal transformation, then use the fact that the Jacobian of a diagonal transformation is the product of the diagonal entries.

\bigskip\noindent\textbf{Solution:}
Let $G(x)=(g_1(x_1),\dots,g_n(x_n))$. Then $h=f\circ G$ and $Dh(x)=Df(G(x))\,DG(x)$ with $DG(x)=\operatorname{diag}(g_1'(x_1),\dots,g_n'(x_n))$. Taking determinants,
\[J_h(x)=J_f(G(x))\,\prod_{k=1}^n g_k'(x_k).
\]\qed


\begin{problembox}[13.4: Polar and Spherical Coordinates]
\begin{problemstatement}
\begin{enumerate}[label=(\alph*)]
    \item If \( x(r, \theta) = r \cos \theta, y(r, \theta) = r \sin \theta \), show that
    \[\frac{\partial (x, y)}{\partial (r, \theta)} = r.\]
    \item If \( x(r, \theta, \phi) = r \cos \theta \sin \phi, y(r, \theta, \phi) = r \sin \theta \sin \phi, z = r \cos \phi \), show that
    \[\frac{\partial (x, y, z)}{\partial (r, \theta, \phi)} = -r^2 \sin \phi.\]
\end{enumerate}
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Compute the partial derivatives directly and evaluate the Jacobian determinants using the standard formulas for coordinate transformations.

\bigskip\noindent\textbf{Solution:}
\begin{enumerate}[label=(\alph*)]
    \item $x_r=\cos\theta,\ x_\theta=-r\sin\theta,\ y_r=\sin\theta,\ y_\theta=r\cos\theta$, so
    $\dfrac{\partial(x,y)}{\partial(r,\theta)}=x_r y_\theta-x_\theta y_r=r$.
    \item With $x=r\cos\theta\sin\phi,\ y=r\sin\theta\sin\phi,\ z=r\cos\phi$, a direct computation gives
    $\dfrac{\partial(x,y,z)}{\partial(r,\theta,\phi)}=-r^2\sin\phi$.
\end{enumerate}\qed


\begin{problembox}[13.5: Implicit Function Theorem Application]
\begin{problemstatement}
\begin{enumerate}[label=(\alph*)]
    \item State conditions on \( f \) and \( g \) which will ensure that the equations \( x = f(u, v), y = g(u, v) \) can be solved for \( u \) and \( v \) in a neighborhood of \( (x_0, y_0) \). If the solutions are \( u = F(x, y), v = G(x, y) \), and if \( J = \partial (f, g)/\partial (u, v) \), show that
    \[\frac{\partial F}{\partial x} = \frac{1}{J} \frac{\partial g}{\partial v}, \quad \frac{\partial F}{\partial y} = -\frac{1}{J} \frac{\partial f}{\partial v}, \quad \frac{\partial G}{\partial x} = -\frac{1}{J} \frac{\partial g}{\partial u}, \quad \frac{\partial G}{\partial y} = \frac{1}{J} \frac{\partial f}{\partial u}.\]
    \item Compute \( J \) and the partial derivatives of \( F \) and \( G \) at \((x_0, y_0) = (1, 1)\) when \( f(u, v) = u^2 - v^2 \), \( g(u, v) = 2uv \).
\end{enumerate}
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Apply the Inverse Function Theorem to establish local invertibility, then use the relationship between the derivatives of inverse functions to derive the partial derivative formulas. For part (b), solve the system of equations to find the preimage point.

\bigskip\noindent\textbf{Solution:}
\begin{enumerate}[label=(\alph*)]
    \item If $f,g\in C^1$ near $(u_0,v_0)$ and $J=\dfrac{\partial(f,g)}{\partial(u,v)}(u_0,v_0)\neq 0$, then by the Inverse Function Theorem the equations $x=f(u,v),\ y=g(u,v)$ can be solved locally as $u=F(x,y),\ v=G(x,y)$. Moreover,
    \[
    \begin{aligned}
    \frac{\partial F}{\partial x}&=\frac{1}{J}\,\frac{\partial g}{\partial v},&\qquad \frac{\partial F}{\partial y}&=-\frac{1}{J}\,\frac{\partial f}{\partial v},\\
    \frac{\partial G}{\partial x}&=-\frac{1}{J}\,\frac{\partial g}{\partial u},&\qquad \frac{\partial G}{\partial y}&=\frac{1}{J}\,\frac{\partial f}{\partial u}.
    \end{aligned}
    \]
    \item Here $f=u^2-v^2,\ g=2uv$. At a point mapping to $(x_0,y_0)=(1,1)$ we have $u^2-v^2=1$ and $2uv=1$. Then
    \[J=\frac{\partial(f,g)}{\partial(u,v)}=\begin{vmatrix}2u&-2v\\2v&2u\end{vmatrix}=4(u^2+v^2)=4\sqrt{(u^2-v^2)^2+(2uv)^2}=4\sqrt{2}.
    \]
    Using the formulas above,
    \begin{align*}
    \frac{\partial F}{\partial x}&=\frac{2u}{J}=\frac{u}{2\sqrt2},\\
    \frac{\partial F}{\partial y}&=\frac{2v}{J}=\frac{v}{2\sqrt2},\\
    \frac{\partial G}{\partial x}&=-\frac{2v}{J}=-\frac{v}{2\sqrt2},\\
    \frac{\partial G}{\partial y}&=\frac{2u}{J}=\frac{u}{2\sqrt2}.
    \end{align*}
    
    Taking the branch with $u,v>0$, $u=\sqrt{\tfrac{1+\sqrt2}{2}}$ and $v=\sqrt{\tfrac{\sqrt2-1}{2}}$.
\end{enumerate}\qed


\begin{problembox}[13.6: Jacobian Matrix Identity]
\begin{problemstatement}
Let \( f \) and \( g \) be related as in Theorem 13.6. Consider the case \( n = 3 \) and show that we have
\[J_i(x)D_1g_i(y) =
\begin{vmatrix}
\delta_{i,1} & D_1f_2(x) & D_1f_3(x)\\
\delta_{i,2} & D_2f_2(x) & D_2f_3(x)\\
\delta_{i,3} & D_3f_2(x) & D_3f_3(x)
\end{vmatrix}
(i = 1, 2, 3),\]
where \( y = f(x) \) and \( \delta_{i,j} = 0 \) or 1 according as \( i \neq j \) or \( i = j \). Use this to deduce the formula
\[D_1g_1 = \frac{\partial (f_2, f_3)}{\partial (x_2, x_3)} \left| \frac{\partial (f_1, f_2, f_3)}{\partial (x_1, x_2, x_3)} \right|.\]
There are similar expressions for the other eight derivatives \( D_kg_l \).
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Use the fact that $Dg(y)=\big(Df(x)\big)^{-1}$ and express the inverse matrix in terms of cofactors, then identify the specific cofactor pattern for each partial derivative.

\bigskip\noindent\textbf{Solution:}
By Theorem 13.6, $Dg(y)=\big(Df(x)\big)^{-1}$ with $y=f(x)$. Writing $Df$ as the $3\times3$ matrix $A=[D_j f_i]$, we have $A^{-1}=\dfrac{1}{\det A}\,\operatorname{adj}(A)$, whose entries are cofactors divided by $J_f(x)$. Identifying the appropriate cofactors yields
\[J_i(x)D_1g_i(y)=\begin{vmatrix}
\delta_{i,1} & D_1f_2 & D_1f_3\\
\delta_{i,2} & D_2f_2 & D_2f_3\\
\delta_{i,3} & D_3f_2 & D_3f_3
\end{vmatrix},\quad i=1,2,3.
\]
Setting $i=1$ and dividing by $J_f=\dfrac{\partial(f_1,f_2,f_3)}{\partial(x_1,x_2,x_3)}$ gives
\[D_1g_1=\frac{\partial(f_2,f_3)}{\partial(x_2,x_3)}\left/\frac{\partial(f_1,f_2,f_3)}{\partial(x_1,x_2,x_3)}\right.\,.
\]
The other eight $D_k g_\ell$ follow similarly.\qed


\begin{problembox}[13.7: Complex Function Properties]
\begin{problemstatement}
Let \( f = u + iv \) be a complex-valued function satisfying the following conditions: \( u \in C' \) and \( v \in C' \) on the open disk \( A = \{z : |z| < 1\}; f \) is continuous on the closed disk \( \bar{A} = \{z : |z| \leq 1\}; u(x, y) = x \) and \( v(x, y) = y \) whenever \( x^2 + y^2 = 1 \); the Jacobian \( J_f(z) > 0 \) if \( z \in A \). Let \( B = f(A) \) denote the image of \( A \) under \( f \) and prove that:
\begin{enumerate}[label=(\alph*)]
    \item If \( X \) is an open subset of \( A \), then \( f(X) \) is an open subset of \( B \).
    \item \( B \) is an open disk of radius 1.
    \item For each point \( u_0 + iv_0 \) in \( B \), there is only a finite number of points \( z \) in \( A \) such that \( f(z) = u_0 + iv_0 \).
\end{enumerate}
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Use the Inverse Function Theorem and invariance of domain to establish local properties, then use compactness and boundary conditions to determine the global structure of the image.

\bigskip\noindent\textbf{Solution:}
Let $A=\{z:\,|z|<1\}$ and $\bar A=\{z:\,|z|\le 1\}$. Since $u,v\in C^1$ and $J_f>0$ on $A$, $f$ is a local $C^1$-diffeomorphism on $A$.
\begin{enumerate}[label=(\alph*)]
    \item If $X\subset A$ is open, each $z\in X$ has a neighborhood on which $f$ is a diffeomorphism, so $f(X)$ is a union of open sets; hence $f(X)$ is open in $B=f(A)$.
    \item On $|z|=1$ we have $u=x,\ v=y$, so $f$ restricts to the identity on the unit circle. By invariance of domain, $B$ is open; since $f$ maps the boundary $|z|=1$ to itself with positive Jacobian in $A$, $B$ must be the open unit disk.
    \item Because $f$ is a local diffeomorphism on $A$, preimages of a point are isolated. If a point of $B$ had infinitely many preimages in the compact set $\bar A$, they would accumulate in $A$, contradicting local injectivity. Thus each value in $B$ has finitely many preimages in $A$.
\end{enumerate}\qed
\section{Extremum Problems}

\subsection*{Essential Definitions and Theorems}

\begin{definition}[Critical Points]
A point $x_0$ is a critical point of a differentiable function $f: \mathbb{R}^n \to \mathbb{R}$ if $\nabla f(x_0) = 0$. At critical points, the function may have local minima, local maxima, or saddle points.
\end{definition}

\noindent\textbf{Importance:} Critical points are the candidates for local extrema of functions. They are essential for optimization problems and understanding the local behavior of functions. The classification of critical points using the Hessian matrix provides a systematic way to determine their nature.



\begin{definition}[Hessian Test]
For a twice-differentiable function $f$ at a critical point $x_0$, the Hessian matrix $H_f(x_0)$ determines the nature of the critical point:
\begin{enumerate}[label=(\alph*)]
\item If $H_f(x_0)$ is positive definite, then $x_0$ is a local minimum
\item If $H_f(x_0)$ is negative definite, then $x_0$ is a local maximum
\item If $H_f(x_0)$ has both positive and negative eigenvalues, then $x_0$ is a saddle point
\end{enumerate}
\end{definition}

\noindent\textbf{Importance:} The Hessian test provides a practical method for classifying critical points. For $2 \times 2$ matrices, we can use $\det H$ and $\operatorname{tr} H$ to determine definiteness. This test is fundamental for optimization algorithms and understanding function behavior.



\begin{theorem}[Lagrange Multipliers]
To find extrema of a function $f: \mathbb{R}^n \to \mathbb{R}$ subject to constraints $g_i(x) = 0$ for $i = 1, \ldots, m$, solve the system:
\[\nabla f(x) = \sum_{i=1}^m \lambda_i \nabla g_i(x)\]
together with the constraint equations $g_i(x) = 0$.
\end{theorem}

\noindent\textbf{Importance:} Lagrange multipliers provide a systematic method for solving constrained optimization problems. The theorem states that at a constrained extremum, the gradient of the objective function is a linear combination of the gradients of the constraints. This is essential for many applications in physics, economics, and engineering.



\begin{theorem}[Cauchy-Schwarz Inequality]
For vectors $a, x \in \mathbb{R}^n$:
\[\left|\sum_{k=1}^n a_k x_k\right| \leq \|a\| \|x\|\]
with equality if and only if $x$ is proportional to $a$.
\end{theorem}

\noindent\textbf{Importance:} This is one of the most fundamental inequalities in analysis. It provides bounds for inner products and is essential for proving many results in optimization, functional analysis, and geometry. The equality condition is crucial for identifying extremal cases.



\begin{theorem}[Arithmetic Mean-Geometric Mean Inequality]
For nonnegative real numbers $t_1, \ldots, t_n$:
\[\frac{t_1 + \cdots + t_n}{n} \geq (t_1 \cdots t_n)^{1/n}\]
with equality if and only if all $t_i$ are equal.
\end{theorem}

\noindent\textbf{Importance:} This inequality relates the arithmetic and geometric means, providing bounds that are essential for many optimization problems and proofs. It's particularly useful for proving other inequalities and for understanding the relationship between different types of averages.




\begin{problembox}[13.8: Extreme Value Classification]
\begin{problemstatement}
Find and classify the extreme values (if any) of the functions defined by the following equations:
\begin{enumerate}[label=(\alph*)]
    \item \( f(x, y) = y^2 + x^2y + x^4 \),
    \item \( f(x, y) = x^2 + y^2 + x + y + xy \),
    \item \( f(x, y) = (x - 1)^4 + (x - y)^4 \),
    \item \( f(x, y) = y^2 - x^3 \).
\end{enumerate}
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Find critical points by setting partial derivatives to zero, then use the Hessian test or direct analysis to classify each critical point as local minimum, maximum, or saddle point.

\bigskip\noindent\textbf{Solution:}
\begin{enumerate}[label=(\alph*)]
    \item $f_x=2x(y+2x^2),\ f_y=2y+x^2$. The only critical point is $(0,0)$. Since $f=y^2+x^2y+x^4\ge 0$ and $f(0,0)=0$, this is a local minimum.
    \item $f_x=2x+y+1,\ f_y=2y+x+1$ gives critical point $(-\tfrac13,-\tfrac13)$. Hessian $\begin{pmatrix}2&1\\1&2\end{pmatrix}$ is positive definite $\Rightarrow$ local (indeed global) minimum.
    \item $f_x=4(x-1)^3+4(x-y)^3,\ f_y=-4(x-y)^3$ gives $(1,1)$. Since $f=(x-1)^4+(x-y)^4\ge0$ with equality only at $(1,1)$, this is a global minimum.
    \item $f_x=-3x^2,\ f_y=2y$ gives $(0,0)$. Along $y=0$, $f=-x^3$ changes sign $\Rightarrow$ saddle.
\end{enumerate}\qed


\begin{problembox}[13.9: Shortest Distance to Parabola]
\begin{problemstatement}
Find the shortest distance from the point \((0, b)\) on the \( y \)-axis to the parabola \( x^2 - 4y = 0 \). Solve this problem using Lagrange's method and also without using Lagrange's method.
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} For the direct method, parameterize the parabola and minimize the distance function. For Lagrange multipliers, minimize the squared distance subject to the parabola constraint, then compare the two approaches.

\bigskip\noindent\textbf{Solution:}
Write the parabola as $y=\tfrac{x^2}{4}$. The squared distance from $(0,b)$ to $(x,\tfrac{x^2}{4})$ is
\[D(x)=x^2+\Big(\tfrac{x^2}{4}-b\Big)^2=\frac{x^4}{16}+x^2-b\frac{x^2}{2}+b^2.
\]
Then $D'(x)=x\Big(\tfrac{x^2}{4}+2-b\Big)$, so critical points are $x=0$ or $x^2=4(b-2)$. Thus: if $b<2$, the only candidate is $x=0$, giving distance $|b|$. If $b\ge 2$, the minimizing points satisfy $x^2=4(b-2)$ and the minimal distance is $\sqrt{4(b-1)}=2\sqrt{b-1}$.
With Lagrange multipliers, minimize $f(x_1,x_2)=x_1^2+(x_2-b)^2$ subject to $g(x)=x_1^2-4x_2=0$. Then $\nabla f=\lambda\nabla g$ gives $(2x_1,2(x_2-b))=\lambda(2x_1,-4)$. Either $x_1=0\Rightarrow x_2=0$ (distance $|b|$), or $\lambda=1\Rightarrow x_2=b-2$ and $x_1^2=4(b-2)$, yielding distance $2\sqrt{b-1}$ when $b\ge2$.\qed


\begin{problembox}[13.10: Geometric Problems]
\begin{problemstatement}
Solve the following geometric problems by Lagrange's method:
\begin{enumerate}[label=(\alph*)]
    \item Find the shortest distance from the point \((a_1, a_2, a_3)\) in \( R^3 \) to the plane whose equation is \( b_1x_1 + b_2x_2 + b_3x_3 + b_0 = 0 \).
    \item Find the point on the line of intersection of the two planes
    \[a_1x_1 + a_2x_2 + a_3x_3 + a_0 = 0\]
    and
    \[b_1x_1 + b_2x_2 + b_3x_3 + b_0 = 0\]
    which is nearest the origin.
\end{enumerate}
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} For part (a), minimize the squared distance function subject to the plane constraint. For part (b), minimize the squared distance to the origin subject to two plane constraints, which reduces to finding the orthogonal projection onto the line of intersection.

\bigskip\noindent\textbf{Solution:}
\begin{enumerate}[label=(\alph*)]
    \item Minimize $\|x-a\|^2$ subject to $b\cdot x+b_0=0$. Lagrange gives $x-a=\lambda b$ and $b\cdot(a+\lambda b)+b_0=0$, hence $\lambda=\dfrac{-(b\cdot a+b_0)}{\|b\|^2}$. The distance is $\dfrac{|b\cdot a+b_0|}{\|b\|}$.
    \item Minimize $\|x\|^2$ subject to $a\cdot x+a_0=0$ and $b\cdot x+b_0=0$. Lagrange gives $x+\mu a+\nu b=0$, so $x=-(\mu a+\nu b)$. Solve
    \[\begin{pmatrix}\|a\|^2 & a\cdot b\\ a\cdot b & \|b\|^2\end{pmatrix}\begin{pmatrix}\mu\\ \nu\end{pmatrix}=\begin{pmatrix}a_0\\ b_0\end{pmatrix},\quad x=-(\mu a+\nu b),\]
    which is the orthogonal projection of the origin onto the line of intersection.
\end{enumerate}\qed


\begin{problembox}[13.11: Maximum Value with Constraint]
\begin{problemstatement}
Find the maximum value of \(| \sum_{k=1}^n a_k x_k |\), if \(\sum_{k=1}^n x_k^2 = 1\), by using 
\begin{enumerate}[label=(\alph*)]
    \item the Cauchy-Schwarz inequality.
    \item Lagrange's method.
\end{enumerate}
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} For part (a), apply Cauchy-Schwarz directly to the sum. For part (b), use Lagrange multipliers to find critical points, then identify the maximum by analyzing the gradient equations.

\bigskip\noindent\textbf{Solution:}
\begin{enumerate}[label=(\alph*)]
    \item By Cauchy--Schwarz, $\big|\sum a_k x_k\big|\le \|a\|\,\|x\|=\|a\|$, with equality when $x$ is proportional to $a$.
    \item Lagrange on $L=\sum a_k x_k-\lambda(\sum x_k^2-1)$ gives $a_k=2\lambda x_k$, so $x\parallel a$ and the maximum is $\|a\|$.
\end{enumerate}\qed


\begin{problembox}[13.12: Maximum of Product under Constraint]
\begin{problemstatement}
Find the maximum of \((x_1 x_2 \cdots x_n)^2\) under the restriction
\[ x_1^2 + \cdots + x_n^2 = 1. \]
Use the result to derive the following inequality, valid for positive real numbers \(a_1, \ldots, a_n\)
\[ (a_1 \cdots a_n)^{1/n} \leq \frac{a_1 + \cdots + a_n}{n}. \]
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Apply the AM-GM inequality to the squares $x_i^2$ under the constraint, then use a change of variables to derive the general AM-GM inequality for positive numbers.

\bigskip\noindent\textbf{Solution:}
By AM--GM on $x_1^2,\dots,x_n^2$ with $\sum x_i^2=1$,
\[(x_1\cdots x_n)^2\le \left(\frac{1}{n}\right)^n,\]
with equality when $|x_1|=\cdots=|x_n|=\tfrac{1}{\sqrt n}$. For $a_i>0$, set $x_i=\dfrac{\sqrt{a_i}}{\sqrt{a_1+\cdots+a_n}}$ to obtain
\[(a_1\cdots a_n)^{1/n}\le \frac{a_1+\cdots+a_n}{n}.
\]\qed


\begin{problembox}[13.13: Local Extremum with Condition]
\begin{problemstatement}
If \(f(x) = x_1^k + \cdots + x_n^k, x = (x_1, \ldots, x_n)\), show that a local extreme of \(f\), subject to the condition \(x_1 + \cdots + x_n = a\), is \(a^k n^{1-k}\).
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Use Lagrange multipliers to find critical points, then show that all variables must be equal at the extremum by analyzing the gradient equations.

\bigskip\noindent\textbf{Solution:}
Use Lagrange multipliers for the constraint \(g(x)=x_1+\cdots+x_n-a=0\). Consider
\[L(x,\lambda)=\sum_{i=1}^n x_i^{\,k}-\lambda\Big(\sum_{i=1}^n x_i-a\Big).\]
Stationarity gives \(k\,x_i^{\,k-1}=\lambda\) for each \(i\), hence all \(x_i\) are equal: \(x_i=t\). The constraint yields \(nt=a\Rightarrow t=a/n\). Therefore
\[f(x)=\sum_{i=1}^n x_i^{\,k}=n\Big(\tfrac{a}{n}\Big)^{\!k}=a^{\,k}n^{\,1-k}.
\]
For \(k>1\) this point gives the constrained minimum (convexity of \(t\mapsto t^{k}\)); for \(0<k<1\) it gives the constrained maximum.\qed


\begin{problembox}[13.14: Local Extremum with Side Conditions]
\begin{problemstatement}
Show that all points \((x_1, x_2, x_3, x_4)\) where \(x_1^2 + x_2^2\) has a local extremum subject to the two side conditions \(x_1^2 + x_3^2 + x_4^2 = 4, x_2^2 + 2x_3^2 + 3x_4^2 = 9\), are found among 
\[ (0, 0, \pm \sqrt{3}, \pm 1), (0, \pm 1, +2, 0), (\pm 1, 0, 0, \pm \sqrt{3}), (\pm 2, \pm 3, 0, 0). \]
Which of these yield a local maximum and which yield a local minimum? Give reasons for your conclusions.
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Evaluate the objective function $x_1^2+x_2^2$ at each of the given feasible points to determine which are extrema, then use the fact that the function is bounded below by 0 and above by the maximum possible value on the constraint set.

\bigskip\noindent\textbf{Solution:}
Evaluating $x_1^2+x_2^2$ at the listed feasible points gives: $(0,0,\pm\sqrt3,\pm1)\mapsto 0$; $(0,\pm1,2,0)\mapsto 1$; $(\pm1,0,0,\pm\sqrt3)\mapsto 1$; $(\pm2,\pm3,0,0)\mapsto 13$. Hence the points with value $0$ yield local (global) minima; those with value $13$ yield local (global) maxima; the points with value $1$ are neither maxima nor minima along the constraint surface.\qed


\begin{problembox}[13.15: Extreme Values with Side Conditions]
\begin{problemstatement}
Show that the extreme values of \(f(x_1, x_2, x_3) = x_1^2 + x_2^2 + x_3^2\), subject to the two side conditions
\[ \sum_{j=1}^3 \sum_{i=1}^3 a_{ij} x_i x_j = 1 \quad (a_{ij} = a_{ji}) \]
and
\[ b_1 x_1 + b_2 x_2 + b_3 x_3 = 0, \quad (b_1, b_2, b_3) \neq (0, 0, 0), \]
are \(t_1^{-1}, t_2^{-1}\), where \(t_1\) and \(t_2\) are the roots of the equation
\[\begin{vmatrix}
b_1 & b_2 & b_3 & 0 \\
a_{11} - t & a_{12} & a_{13} & b_1 \\
a_{21} & a_{22} - t & a_{23} & b_2 \\
a_{31} & a_{32} & a_{33} - t & b_3
\end{vmatrix} = 0.\]
Show that this is a quadratic equation in \(t\) and give a geometric argument to explain why the roots \(t_1, t_2\) are real and positive.
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Use Lagrange multipliers for both constraints, then eliminate the multipliers to obtain a system that leads to the determinant equation. The geometric interpretation involves finding the intersection of an ellipsoid with a plane and identifying the extreme distances from the origin.

\bigskip\noindent\textbf{Solution:}
Introduce multipliers $\lambda,\mu$ for the constraints $x^{\!T}Ax=1$ and $b^{\!T}x=0$. Then
\[2x=2\lambda Ax+\mu b,\quad x^{\!T}Ax=1,\quad b^{\!T}x=0.
\]
Assuming $x\neq 0$, rearrange to $(I-\lambda A)x=\tfrac{\mu}{2}\,b$. Nontrivial solutions exist only when
\[\begin{vmatrix}
0 & b^{\!T} \\
b & A-\lambda^{-1}I
\end{vmatrix}=0.
\]
With $t=\lambda^{-1}$ this equals
\[\begin{vmatrix}
b_1 & b_2 & b_3 & 0 \\
a_{11} - t & a_{12} & a_{13} & b_1 \\
a_{21} & a_{22} - t & a_{23} & b_2 \\
a_{31} & a_{32} & a_{33} - t & b_3
\end{vmatrix}=0,
\]
which is quadratic in $t$. The corresponding extremal values of $f=\|x\|^2$ are $t_1^{-1}, t_2^{-1}$. Geometrically, the feasible set is (generically) a line in the ellipsoid $x^{\!T}Ax=1$, so the two tangent points exist, making $t_1,t_2$ real and positive.\qed


\begin{problembox}[13.16: Hadamard's Theorem]
\begin{problemstatement}
Let \(\Delta = \det [x_{ij}]\) and let \(X_i = (x_{i1}, \ldots, x_{in})\). A famous theorem of Hadamard states that \(|\Delta| \leq d_1 \cdots d_n\), if \(d_1, \ldots, d_n\) are \(n\) positive constants such that \(\| X_i \|^2 = d_i^2 (i = 1, 2, \ldots, n)\). Prove this by treating \(\Delta\) as a function of \(n^2\) variables subject to \(n\) constraints, using Lagrange's method to show that, when \(\Delta\) has an extreme under these conditions, we must have
\[\Delta^2 = 
\begin{vmatrix}
d_1^2 & 0 & 0 & \cdots & 0 \\
0 & d_2^2 & 0 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & \cdots & d_n^2
\end{vmatrix}.\]
\end{problemstatement}
\end{problembox}

\noindent\textbf{Strategy:} Use Lagrange multipliers to find critical points of the determinant function subject to the row norm constraints, then show that at an extremum the rows must be orthogonal, which leads to the diagonal form of the matrix.

\bigskip\noindent\textbf{Solution:}
Let the rows be $X_i\in\mathbb R^n$ with $\|X_i\|=d_i$. Consider $\Delta^2=\det(XX^{\!T})$, where $X=[x_{ij}]$. Under the constraints $\|X_i\|=d_i$, Lagrange multipliers show that at an extremum the rows are pairwise orthogonal. Then $XX^{\!T}=\operatorname{diag}(d_1^2,\dots,d_n^2)$ and
\[\Delta^2=\det(XX^{\!T})=d_1^2\cdots d_n^2,\]
which yields $|\Delta|\le d_1\cdots d_n$ with equality exactly when the rows are orthogonal.

\section{Solving and Proving Techniques}

\subsection*{Working with Jacobians}
\begin{itemize}
\item Use the definition: $J_f(x) = \det[Df(x)]$ where $Df(x)$ is the derivative matrix
\item Apply the chain rule: $J_{f \circ g}(x) = J_f(g(x)) \cdot J_g(x)$
\item Use the matrix determinant lemma: $\det(I + uv^T) = 1 + v^T u$
\item Apply the fact that Jacobians are multiplicative under composition
\item Use the fact that the Jacobian of the inverse function is the reciprocal of the original Jacobian
\end{itemize}

\subsection*{Proving Invertibility}
\begin{itemize}
\item Use the Inverse Function Theorem: if $f \in C^1$ and $J_f(x_0) \neq 0$, then $f$ is locally invertible near $x_0$
\item Show that the function is one-to-one by solving for the inverse explicitly
\item Use the fact that if $f$ is invertible, then $Df^{-1}(f(x_0)) = [Df(x_0)]^{-1}$
\item Apply the fact that continuous bijections on compact sets have continuous inverses
\item Use the fact that monotone functions are invertible
\end{itemize}

\subsection*{Working with Coordinate Transformations}
\begin{itemize}
\item Use the fact that polar coordinates have Jacobian $r$: $\frac{\partial(x,y)}{\partial(r,\theta)} = r$
\item Apply the fact that spherical coordinates have Jacobian $-r^2 \sin \phi$
\item Use the fact that coordinate transformations preserve volumes up to the Jacobian factor
\item Apply the fact that the Jacobian of a composition is the product of Jacobians
\item Use the fact that the Jacobian of the identity transformation is 1
\end{itemize}

\subsection*{Applying the Implicit Function Theorem}
\begin{itemize}
\item Use the fact that if $f \in C^1$ and the Jacobian matrix is invertible, then the system can be solved locally
\item Apply the fact that partial derivatives of implicit functions can be computed using the chain rule
\item Use the fact that the implicit function theorem provides local existence and uniqueness
\item Apply the fact that the derivatives of implicit functions can be found by differentiating the defining equations
\item Use the fact that the implicit function theorem can be used to solve systems of equations
\end{itemize}

\subsection*{Working with Lagrange Multipliers}
\begin{itemize}
\item Use the fact that at a constrained extremum, the gradient of the objective function is a linear combination of the gradients of the constraints
\item Apply the fact that Lagrange multipliers can be used to find critical points of functions subject to constraints
\item Use the fact that the number of Lagrange multipliers equals the number of constraints
\item Apply the fact that Lagrange multipliers can be used to prove inequalities like AM-GM
\item Use the fact that Lagrange multipliers can be used to find extreme values on constraint surfaces
\end{itemize}

\subsection*{Proving Inequalities}
\begin{itemize}
\item Use the AM-GM inequality: $(a_1 \cdots a_n)^{1/n} \leq \frac{a_1 + \cdots + a_n}{n}$
\item Apply Lagrange multipliers to find extreme values under constraints
\item Use the fact that continuous functions on compact sets attain their extrema
\item Apply the fact that convex functions have unique global minima
\item Use the fact that the arithmetic mean is always greater than or equal to the geometric mean
\end{itemize}