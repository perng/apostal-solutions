\chapter{Multivariable Differential Calculus}

\section{Differentiable Functions}

\noindent\textbf{Tools for this section.} Definitions of partial derivatives $D_k f$, directional derivatives $f'(x;u)$, Fr√©chet differentiability and linear approximation; linearity of the derivative; relation $f'(x;u)=Df(x)\,u$ when $f$ is differentiable; Jacobian/derivative matrix; basic sum/product rules.



\begin{problembox}[12.1: Local Extrema and Partial Derivatives]
Let \( S \) be an open subset of \( \mathbb{R}^n \), and let \( f: S \to \mathbb{R} \) be a real-valued function with finite partial derivatives \( D_1f, \ldots, D_nf \) on \( S \). If \( f \) has a local maximum or a local minimum at a point \( c \) in \( S \), prove that \( D_kf(c) = 0 \) for each \( k \).
\end{problembox}

\noindent\textbf{Strategy:} Use the definition of partial derivatives by restricting to coordinate directions. For each coordinate direction, the function becomes a one-variable function that has a local extremum at the origin, so its derivative must be zero.

\bigskip\noindent\textbf{Solution:}
Fix $k$ and set $\phi(t)=f(c+te_k)$ for small $t$. If $c$ is a local extremum, then $t=0$ is a local extremum of the one-variable function $\phi$, hence $\phi'(0)=0$. But $\phi'(0)=D_k f(c)$ by definition, so $D_k f(c)=0$ for each $k$.\qed


\begin{problembox}[12.2: Partial and Directional Derivatives]
Calculate all first-order partial derivatives and the directional derivative \( f'(x; u) \) for each of the real-valued functions defined on \( \mathbb{R}^n \) as follows:
\begin{enumerate}[label=(\alph*)]
\item \( f(x) = a \cdot x \), where \( a \) is a fixed vector in \( \mathbb{R}^n \).
\item \( f(x) = \|x\|^4 \).
\item \( f(x) = x \cdot L(x) \), where \( L : \mathbb{R}^n \to \mathbb{R}^n \) is a linear function.
\item \( f(x) = \sum_{i=1}^{n} \sum_{j=1}^{n} a_{ij}x_i x_j \), where \( a_{ij} = a_{ji} \).
\end{enumerate}
\end{problembox}

\noindent\textbf{Strategy:} Compute partial derivatives using standard differentiation rules, then use the relation \( f'(x; u) = \nabla f(x) \cdot u \) to find directional derivatives. For linear functions, use matrix notation; for quadratic forms, use the symmetry property.

\bigskip\noindent\textbf{Solution:}
\begin{enumerate}[label=(\alph*)]
\item $D_i f(x)=a_i$, so $\nabla f=a$. Hence $f'(x;u)=a\cdot u$.
\item Write $r^2=\|x\|^2$. Then $D_i f(x)=4r^2 x_i$ and $\nabla f(x)=4r^2 x$. Thus $f'(x;u)=\nabla f\cdot u=4\|x\|^2(x\cdot u)$.
\item Let $L(x)=Ax$ for some matrix $A$. Then $f(x)=x\cdot Ax$ and $\nabla f(x)=(A+A^{\!\top})x$. Thus $D_i f(x)=[(A+A^{\!\top})x]_i$ and $f'(x;u)=u\cdot (A+A^{\!\top})x$.
\item With $A=(a_{ij})$ symmetric, $f(x)=x^{\!\top}Ax$, so $\nabla f(x)=2Ax$, $D_i f(x)=[2Ax]_i$, and $f'(x;u)=2u\cdot Ax$.
\end{enumerate}\qed


\begin{problembox}[12.3: Directional Derivatives of Sum and Product]
Let \( f \) and \( g \) be functions with values in \( \mathbb{R}^m \) such that the directional derivatives \( f'(c; u) \) and \( g'(c; u) \) exist. Prove that the sum \( f + g \) and dot product \( f \cdot g \) have directional derivatives given by
\[(f + g)'(c; u) = f'(c; u) + g'(c; u)\]
and
\[(f \cdot g)'(c; u) = f(c) \cdot g'(c; u) + g(c) \cdot f'(c; u).\]
\end{problembox}

\noindent\textbf{Strategy:} Use the definition of directional derivatives as limits of difference quotients. For the sum, apply linearity of limits. For the product, expand the difference quotient and use the product rule for dot products.

\bigskip\noindent\textbf{Solution:}
For $f+g$, divide the increment $[f(c+tu)+g(c+tu)]-[f(c)+g(c)]$ by $t$ and pass to the limit. For $f\cdot g$, expand
\begin{align*}
&\frac{f(c+tu)\cdot g(c+tu)-f(c)\cdot g(c)}{t} \\
=&f(c)\cdot\frac{g(c+tu)-g(c)}{t}+g(c)\cdot\frac{f(c+tu)-f(c)}{t}+o(1),
\end{align*}
and take $t\to0$.\qed


\begin{problembox}[12.4: Differentiability of Vector-Valued Functions]
If \( S \subseteq \mathbb{R}^n \), let \( f: S \to \mathbb{R}^m \) be a function with values in \( \mathbb{R}^m \), and write \( f = (f_1, \ldots, f_m) \). Prove that \( f \) is differentiable at an interior point \( c \) of \( S \) if, and only if, each \( f_i \) is differentiable at \( c \).
\end{problembox}

\noindent\textbf{Strategy:} Use the component-wise definition of differentiability. If \( f \) is differentiable, project to each component to show each \( f_i \) is differentiable. Conversely, if each \( f_i \) is differentiable, construct the Jacobian matrix and show the remainder term is \( o(\|h\|) \).

\bigskip\noindent\textbf{Solution:}
If $f$ is differentiable, then $f(c+h)=f(c)+Df(c)h+o(\|h\|)$; projecting to the $i$th coordinate gives the differentiability of $f_i$ with derivative the $i$th row of $Df(c)$. Conversely, if each $f_i$ is differentiable, stack their linear maps to form the Jacobian $Df(c)$ and note $\|f(c+h)-f(c)-Df(c)h\|\le\sum_i |f_i(c+h)-f_i(c)-Df_i(c)h|=o(\|h\|)$.\qed


\begin{problembox}[12.5: Differentiability of Sum of Univariate Functions]
Given \( n \) real-valued functions \( f_1, \ldots, f_n \), each differentiable on an open interval \( (a, b) \) in \( \mathbb{R} \). For each \( x = (x_1, \ldots, x_n) \) in the \( n \)-dimensional open interval
\[S = \{(x_1, \ldots, x_n): a < x_k < b, \quad k = 1, 2, \ldots, n\},\]
define \( f(x) = f_1(x_1) + \cdots + f_n(x_n) \). Prove that \( f \) is differentiable at each point of \( S \) and that
\[f'(x)(u) = \sum_{i=1}^{n} f_i'(x_i)u_i, \quad \text{where } u = (u_1, \ldots, u_n).\]
\end{problembox}

\noindent\textbf{Strategy:} Since each term depends only on one coordinate, the Jacobian matrix is diagonal. Use the fact that each \( f_i \) is differentiable in one variable to show the remainder term is \( o(\|u\|) \).

\bigskip\noindent\textbf{Solution:}
Each term depends on one coordinate, so $Df(x)$ is diagonal with entries $f_i'(x_i)$. Hence $f'(x)(u)=\sum_i f_i'(x_i)u_i$ and the remainder is $o(\|u\|)$ by the 1D differentiability of each $f_i$.\qed


\begin{problembox}[12.6: Differentiability with Partial Limits]
Given \( n \) real-valued functions \( f_1, \ldots, f_n \) defined on an open set \( S \) in \( \mathbb{R}^n \). For each \( x \) in \( S \), define \( f(x) = f_1(x) + \cdots + f_n(x) \). Assume that for each \( k = 1, 2, \ldots, n \), the following limit exists:
\[\lim_{\substack{y \to x \\ y_k \neq x_k}} \frac{f_k(y) - f_k(x)}{y_k - x_k}.\]
Call this limit \( a_k(x) \). Prove that \( f \) is differentiable at \( x \) and that
\[f'(x)(u) = \sum_{k=1}^{n} a_k(x) u_k \quad \text{if } u = (u_1, \ldots, u_n).\]
\end{problembox}

\noindent\textbf{Strategy:} Approximate the change in \( f \) by varying coordinates one at a time, using the given partial limits. This creates a telescoping sum that gives the linear approximation with remainder term \( o(\|y-x\|) \).

\bigskip\noindent\textbf{Solution:}
Vary $y$ from $x$ by changing one coordinate at a time: $x=x^{(0)}\to x^{(1)}\to\cdots\to x^{(n)}=y$, where only the $k$th coordinate changes in step $k$. Then
\[f(y)-f(x)=\sum_{k=1}^n\big(f_k(x^{(k)})-f_k(x^{(k-1)})\big)=\sum_{k=1}^n a_k(x)(y_k-x_k)+o(\|y-x\|),\]
by the defining limits for $a_k(x)$. Hence $f$ is differentiable with $Df(x)u=\sum_k a_k(x)u_k$.\qed


\begin{problembox}[12.7: Differentiability of Product at Zero]
Let \( f \) and \( g \) be functions from \( \mathbb{R}^n \) to \( \mathbb{R}^m \). Assume that \( f \) is differentiable at \( c \), that \( f(c) = 0 \), and that \( g \) is continuous at \( c \). Let \( h(x) = g(x) \cdot f(x) \). Prove that \( h \) is differentiable at \( c \) and that
\[h'(c)(u) = g(c) \cdot \{f'(c)(u)\} \quad \text{if } u \in \mathbb{R}^n.\]
\end{problembox}

\noindent\textbf{Strategy:} Use the linear approximation for \( f \) at \( c \) and the continuity of \( g \) to expand \( h(c+h) - h(c) \). The key insight is that \( f(c) = 0 \) simplifies the product rule.

\bigskip\noindent\textbf{Solution:}
Write $f(c+h)=f(c)+f'(c)h+r(h)$ with $\|r(h)\|=o(\|h\|)$ and use continuity of $g$:
\[h(c+h)-h(c)=g(c+h)\cdot f'(c)h+g(c+h)\cdot r(h)=g(c)\cdot f'(c)h+o(\|h\|).\]
Thus $h$ is differentiable with derivative $u\mapsto g(c)\cdot f'(c)u$.\qed


\begin{problembox}[12.8: Jacobian Matrix Calculation]
Let \( f : \mathbb{R}^2 \to \mathbb{R}^3 \) be defined by the equation
\[f(x, y) = (\sin x \cos y, \sin x \sin y, \cos x \cos y).\]
Determine the Jacobian matrix \( Df(x, y) \).
\end{problembox}

\noindent\textbf{Strategy:} Compute partial derivatives of each component function with respect to \( x \) and \( y \) using standard differentiation rules for trigonometric functions and the product rule.

\bigskip\noindent\textbf{Solution:}
\[Df(x,y)=\begin{pmatrix}
\cos x\cos y & -\sin x\sin y\\
\cos x\sin y & \sin x\cos y\\
-\sin x\cos y & -\cos x\sin y
\end{pmatrix}.\]\qed


\begin{problembox}[12.9: Nonexistence of Positive Directional Derivative]
Prove that there is no real-valued function \( f \) such that \( f'(c; u) > 0 \) for a fixed point \( c \) in \( \mathbb{R}^n \) and every nonzero vector \( u \) in \( \mathbb{R}^n \). Give an example such that \( f'(c; u) > 0 \) for a fixed direction \( u \) and every \( c \) in \( \mathbb{R}^n \).
\end{problembox}

\noindent\textbf{Strategy:} Use the fact that directional derivatives are linear in the direction vector, so \( f'(c; -u) = -f'(c; u) \). This creates a contradiction if all directional derivatives are positive. For the example, use a linear function.

\bigskip\noindent\textbf{Solution:}
For any $u\ne0$, the 1D definition along lines gives $f'(c;-u)=-f'(c;u)$, so $f'(c;u)>0$ cannot hold for both $u$ and $-u$. For the example with a fixed $u$, take $f(x)=u\cdot x$. Then $f'(c;u)=\|u\|^2>0$ for every $c$.\qed


\begin{problembox}[12.10: Complex Differentiability and Directional Derivatives]
Let \( f = u + iv \) be a complex-valued function such that the derivative \( f'(c) \) exists for some complex \( c \). Write \( z = c + re^{i\alpha} \) (where \( \alpha \) is real and fixed) and let \( r \to 0 \) in the difference quotient \( [f(z) - f(c)]/(z - c) \) to obtain
\[f'(c) = e^{-i\alpha}[u'(c; a) + iv'(c; a)],\]
where \( a = (\cos \alpha, \sin \alpha) \), and \( u'(c; a) \) and \( v'(c; a) \) are directional derivatives. Let \( b = (\cos \beta, \sin \beta) \), where \( \beta = \alpha + \frac{1}{2}\pi \), and show by a similar argument that
\[f'(c) = e^{-i\alpha}[v'(c; b) - iu'(c; b)].\]
Deduce that \( u'(c; a) = v'(c; b) \) and \( v'(c; a) = -u'(c; b) \). The Cauchy-Riemann equations (Theorem 5.22) are a special case.
\end{problembox}

\noindent\textbf{Strategy:} Express the complex derivative in terms of directional derivatives by taking limits along different directions. Use the fact that the complex derivative must be the same regardless of the approach direction, then equate the two expressions to derive the Cauchy-Riemann relations.

\bigskip\noindent\textbf{Solution:}
Along $z=c+re^{i\alpha}$, the complex difference quotient tends to $f'(c)$, while its real and imaginary parts are $u'(c;a)$ and $v'(c;a)$, giving the first identity. Rotating the approach by $\pi/2$ yields the second. Equating the two expressions for $f'(c)$ gives $u'(c;a)=v'(c;b)$ and $v'(c;a)=-u'(c;b)$, which specialize to the Cauchy‚ÄìRiemann equations in Cartesian directions.\qed
\section{Gradients and the Chain Rule}

\noindent\textbf{Tools for this section.} Gradient $\nabla f$, Cauchy‚ÄìSchwarz and maximization of $u\mapsto \nabla f\cdot u$ over unit $u$; multivariable chain rule; product/quotient rules for gradients; polar coordinate relations; compositions with $g_1,g_2$.



\begin{problembox}[12.11: Maximum Directional Derivative]
Let \( f \) be real-valued and differentiable at a point \( c \) in \( \mathbb{R}^n \), and assume that \( \| \nabla f(c) \| \neq 0 \). Prove that there is one and only one unit vector \( u \) in \( \mathbb{R}^n \) such that \( |f'(c; u)| = \| \nabla f(c) \| \), and that this is the unit vector for which \( |f'(c; u)| \) has its maximum value.
\end{problembox}

\noindent\textbf{Strategy:} Use the relation \( f'(c; u) = \nabla f(c) \cdot u \) and apply the Cauchy-Schwarz inequality to find the maximum value. The maximum occurs when \( u \) is parallel to \( \nabla f(c) \).

\bigskip\noindent\textbf{Solution:}
$f'(c;u)=\nabla f(c)\cdot u$. By Cauchy‚ÄìSchwarz, $|\nabla f\cdot u|\le\|\nabla f\|\,\|u\|=\|\nabla f\|$, with equality iff $u=\pm\dfrac{\nabla f}{\|\nabla f\|}$. The unique unit vector maximizing $f'(c;u)$ is $u=\dfrac{\nabla f}{\|\nabla f\|}$.\qed


\begin{problembox}[12.12: Gradient Calculations]
Compute the gradient vector \( \nabla f(x, y) \) at those points \( (x, y) \) in \( \mathbb{R}^2 \) where it exists:
\begin{enumerate}[label=(\alph*)]
\item \( f(x, y) = x^2 y^2 \log (x^2 + y^2) \) if \( (x, y) \ne (0, 0) \), \( f(0, 0) = 0 \).
\item \( f(x, y) = xy \sin \frac{1}{x^2 + y^2} \) if \( (x, y) \ne (0, 0) \), \( f(0, 0) = 0 \).
\end{enumerate}
\end{problembox}

\noindent\textbf{Strategy:} Compute partial derivatives using standard differentiation rules. At the origin, check if the function is differentiable by examining the limit definition, since having partial derivatives does not guarantee differentiability.

\bigskip\noindent\textbf{Solution:}
\begin{enumerate}[label=(\alph*)]
\item For $(x,y)\ne(0,0)$ with $r^2=x^2+y^2$,
\[\nabla f(x,y)=\big(2xy^2\log r^2+\tfrac{2x^3y^2}{r^2},\;2x^2y\log r^2+\tfrac{2x^2y^3}{r^2}\big).\]
At $(0,0)$, $\partial f/\partial x=\partial f/\partial y=0$, but $f$ is not differentiable there; thus $\nabla f$ does not exist at $(0,0)$.
\item For $(x,y)\ne(0,0)$ with $r^2=x^2+y^2$,
\[\nabla f(x,y)=\big(y\sin\tfrac{1}{r^2}-\tfrac{2x^2y}{r^4}\cos\tfrac{1}{r^2},\;x\sin\tfrac{1}{r^2}-\tfrac{2xy^2}{r^4}\cos\tfrac{1}{r^2}\big).\]
At $(0,0)$, the partials are $0$, but $f$ is not differentiable; hence $\nabla f$ does not exist at $(0,0)$.
\end{enumerate}\qed


\begin{problembox}[12.13: Second Order Partials of Composition]
Let \( f \) and \( g \) be real-valued functions defined on \( \mathbb{R}^1 \) with continuous second derivatives \( f'' \) and \( g'' \). Define
\[F(x, y) = f[x + g(y)] \text{ for each } (x, y) \text{ in } \mathbb{R}^2.\]
Find formulas for all partials of \( F \) of first and second order in terms of the derivatives of \( f \) and \( g \). Verify the relation
\[(D_1F)(D_{1,2}F) = (D_2F)(D_{1,1}F).\]
\end{problembox}

\noindent\textbf{Strategy:} Apply the chain rule repeatedly to find first and second order partial derivatives. Use the fact that \( F \) depends on \( x \) and \( y \) only through the single variable \( x + g(y) \), which simplifies the calculations.

\bigskip\noindent\textbf{Solution:}
Let $h(x,y)=x+g(y)$. Then 
\[D_1F=f'(h),\quad D_2F=f'(h)g'(y).\] 
For second order: 
$D_{1,1}F=f''(h)$, $D_{1,2}F=f''(h)g'(y)$, $D_{2,2}F=f''(h)[g'(y)]^2+f'(h)g''(y)$. Then $$(D_1F)(D_{1,2}F)=f'(h)f''(h)g'(y)=(D_2F)(D_{1,1}F)$$.\qed


\begin{problembox}[12.14: Polar Coordinate Transformation]
Given a function \( f \) defined in \( \mathbb{R}^2 \). Let
\[F(r, \theta) = f(r \cos \theta, r \sin \theta).\]
\begin{enumerate}[label=(\alph*)]
\item Assume appropriate differentiability properties of \( f \) and show that
\begin{align*}
D_1F(r, \theta) =& \cos \theta D_1f(x, y) + \sin \theta D_2f(x, y),\\
D_{1,1}F(r, \theta) =& \cos^2 \theta D_{1,1}f(x, y) + 2 \sin \theta \cos \theta D_{1,2}f(x, y) + \\
& \sin^2 \theta D_{2,2}f(x, y),
\end{align*}
where \( x = r \cos \theta, y = r \sin \theta \).
\item Find similar formulas for \( D_2F, D_{1,2}F, \) and \( D_{2,2}F \).
\item Verify the formula
\[\| \nabla f(r \cos \theta, r \sin \theta) \|^2 = [D_1F(r, \theta)]^2 + \frac{1}{r^2} [D_2F(r, \theta)]^2.\]
\end{enumerate}
\end{problembox}

\noindent\textbf{Strategy:} Apply the multivariable chain rule with the coordinate transformation \( x = r \cos \theta, y = r \sin \theta \). For second derivatives, apply the chain rule twice. The gradient formula follows from expressing the Cartesian gradient in polar coordinates.

\bigskip\noindent\textbf{Solution:}
Apply the chain rule with $x=r\cos\theta$, $y=r\sin\theta$. Part (a) follows by differentiating $F(r,\theta)=f(x,y)$ with respect to $r$ twice. Similarly,
\[D_2F=-r\sin\theta\,D_1f+r\cos\theta\,D_2f,\quad D_{1,2}F=-\sin\theta\,D_{1,1}f+\cos\theta\,D_{1,2}f+\cos\theta\,D_{2,1}f+\sin\theta\,D_{2,2}f,\]
and
\[D_{2,2}F=\sin^2\!\theta\,D_{1,1}f-2\sin\theta\cos\theta\,D_{1,2}f+\cos^2\!\theta\,D_{2,2}f.\]
The gradient identity is the standard polar expression $\|\nabla f\|^2=F_r^2+\tfrac{1}{r^2}F_\theta^2$.\qed


\begin{problembox}[12.15: Gradient of Product and Quotient]
If \( f \) and \( g \) have gradient vectors \( \nabla f(x) \) and \( \nabla g(x) \) at a point \( x \) in \( \mathbb{R}^n \) show that the product function \( h \) defined by \( h(x) = f(x)g(x) \) also has a gradient vector at \( x \) and that
\[\nabla h(x) = f(x)\nabla g(x) + g(x)\nabla f(x).\]
State and prove a similar result for the quotient \( f/g \).
\end{problembox}

\noindent\textbf{Strategy:} Apply the product rule for differentiation to each component of the gradient. For the quotient, use the quotient rule and express the result in terms of gradients.

\bigskip\noindent\textbf{Solution:}
From the product rule, $\nabla(fg)=f\,\nabla g+g\,\nabla f$. If $g(x)\ne0$, then $\nabla\!\left(\frac{f}{g}\right)=\dfrac{g\,\nabla f-f\,\nabla g}{g^2}$.\qed


\begin{problembox}[12.16: Gradient of Composition]
Let \( f \) be a function having a derivative \( f' \) at each point in \( \mathbb{R}^1 \) and let \( g \) be defined on \( \mathbb{R}^3 \) by the equation
\[g(x, y, z) = x^2 + y^2 + z^2.\]
If \( h \) denotes the composite function \( h = f \circ g \), show that
\[\| \nabla h(x, y, z) \|^2 = 4g(x, y, z)[f'[g(x, y, z)]]^2.\]
\end{problembox}

\noindent\textbf{Strategy:} Apply the chain rule to find \( \nabla h = f'(g) \nabla g \), then compute the squared norm using the fact that \( g = x^2 + y^2 + z^2 \).

\bigskip\noindent\textbf{Solution:}
$\nabla h=f'(g)\,\nabla g=f'(g)\,(2x,2y,2z)$, so $\|\nabla h\|^2=4(x^2+y^2+z^2)[f'(g)]^2=4g\,[f'(g)]^2$.\qed


\begin{problembox}[12.17: Gradient of Vector-Valued Composition]
Assume \( f \) is differentiable at each point \( (x, y) \) in \( \mathbb{R}^2 \). Let \( g_1 \) and \( g_2 \) be defined on \( \mathbb{R}^3 \) by the equations
\[g_1(x, y, z) = x^2 + y^2 + z^2, \quad g_2(x, y, z) = x + y + z,\]
and let \( g \) be the vector-valued function whose values (in \( \mathbb{R}^2 \)) are given by
\[g(x, y, z) = (g_1(x, y, z), g_2(x, y, z)).\]
Let \( h \) be the composite function \( h = f \circ g \) and show that
\[\| \nabla h \|^2 = 4(D_1f)^2g_1 + 4(D_1f)(D_2f)g_2 + 3(D_2f)^2.\]
\end{problembox}

\noindent\textbf{Strategy:} Apply the multivariable chain rule to express \( \nabla h \) in terms of the partial derivatives of \( f \) and the gradients of \( g_1 \) and \( g_2 \). Then compute the squared norm using the dot product formula.

\bigskip\noindent\textbf{Solution:}
By the chain rule, $\nabla h=(D_1 f)\,\nabla g_1+(D_2 f)\,\nabla g_2$, where $\nabla g_1=(2x,2y,2z)$ and $\nabla g_2=(1,1,1)$. Hence
\begin{align*}
\|\nabla h\|^2&=(D_1 f)^2\|\nabla g_1\|^2+2(D_1 f)(D_2 f)\,\nabla g_1\!\cdot\!\nabla g_2+(D_2 f)^2\|\nabla g_2\|^2\\
&=4(D_1 f)^2 g_1+4(D_1 f)(D_2 f) g_2+3(D_2 f)^2.
\end{align*}\qed


\begin{problembox}[12.18: Euler's Theorem for Homogeneous Functions]
Let \( f \) be defined on an open set \( S \) in \( \mathbb{R}^n \). We say that \( f \) is homogeneous of degree \( p \) over \( S \) if \( f(\lambda x) = \lambda^p f(x) \) for every real \( \lambda \) and for every \( x \) in \( S \) for which \( \lambda x \in S \). If such a function is differentiable at \( x \), show that
\[x \cdot \nabla f(x) = p f(x).\]
NOTE. This is known as Euler's theorem for homogeneous functions. Hint. For fixed \( x \), define \( g(\lambda) = f(\lambda x) \) and compute \( g'(1) \).

Also prove the converse. That is, show that if \( x \cdot \nabla f(x) = p f(x) \) for all \( x \) in an open set \( S \), then \( f \) must be homogeneous of degree \( p \) over \( S \).
\end{problembox}

\noindent\textbf{Strategy:} For the forward direction, use the hint to define \( g(\lambda) = f(\lambda x) \) and apply the chain rule to find \( g'(1) \). For the converse, treat the equation as a differential equation and solve it to show homogeneity.

\bigskip\noindent\textbf{Solution:}
If $f(\lambda x)=\lambda^p f(x)$, then with $g(\lambda)=f(\lambda x)$ we have $g'(1)=x\cdot\nabla f(x)=p f(x)$. Conversely, if $x\cdot\nabla f(x)=p f(x)$ on a star-shaped domain, fix $x$ and solve the ODE $\tfrac{d}{d\lambda}f(\lambda x)=x\cdot\nabla f(\lambda x)=p f(\lambda x)$ to get $f(\lambda x)=\lambda^p f(x)$.\qed
\section{Mean-Value Theorems}

\noindent\textbf{Tools for this section.} One-variable Mean Value Theorem; mean-value form along line segments $t\mapsto x+t(y-x)$; the vector mean-value identity applied componentwise and after taking dot products.



\begin{problembox}[12.19: Mean-Value Theorem for Vector Functions]
Let \( f: \mathbb{R} \rightarrow \mathbb{R}^2 \) be defined by the equation \( f(t) = (\cos t, \sin t) \). Then \( f'(t)(u) = u(-\sin t, \cos t) \) for every real \( u \). The Mean-Value formula
\[f(y) - f(x) = f'(z)(y - x)\]
cannot hold when \( x = 0, y = 2\pi \), since the left member is zero and the right member is a vector of length \( 2\pi \). Nevertheless, Theorem 12.9 states that for every vector \( a \) in \( \mathbb{R}^2 \) there is a \( z \) in the interval \( (0, 2\pi) \) such that
\[a \cdot (f(y) - f(x)) = a \cdot (f'(z)(y - x)).\]
Determine \( z \) in terms of \( a \) when \( x = 0 \) and \( y = 2\pi \).
\end{problembox}

\noindent\textbf{Strategy:} Since \( f(y) - f(x) = 0 \), the equation becomes \( a \cdot f'(z) = 0 \). Use the expression for \( f'(z) \) to find the angle \( z \) that makes this dot product zero.

\bigskip\noindent\textbf{Solution:}
Here $f(y)-f(x)=0$, so $a\cdot f'(z)(2\pi)=0$. Since $f'(z)=(-\sin z,\cos z)$, we require $a_x(-\sin z)+a_y\cos z=0$, i.e., $\tan z=\dfrac{a_y}{a_x}$. Thus $z=\arg(a)\pmod{\pi}$.\qed


\begin{problembox}[12.20: Mean-Value Theorem in Two Variables]
Let \( f \) be a real-valued function differentiable on a 2-ball \( B(x) \). By considering the function
\[g(t) = f[ty_1 + (1 - t)x_1, y_2] + f[x_1, ty_2 + (1 - t)x_2]\]
prove that
\[f(y) - f(x) = (y_1 - x_1)D_1f(z_1, y_2) + (y_2 - x_2)D_2f(x_1, z_2),\]
where \( z_1 \in L(x_1, y_1) \) and \( z_2 \in L(x_2, y_2) \).
\end{problembox}

\noindent\textbf{Strategy:} Apply the one-variable Mean Value Theorem to the function \( g(t) \) on the interval \( [0,1] \). Differentiate \( g \) with respect to \( t \) and use the chain rule to express the result in terms of partial derivatives.

\bigskip\noindent\textbf{Solution:}
Apply the 1D MVT to $g$ on $[0,1]$ to get $g'(\theta)=g(1)-g(0)=f(y)-f(x)$. Differentiate $g$ and collect terms to obtain
\[f(y)-f(x)=(y_1-x_1)D_1 f(z_1,y_2)+(y_2-x_2)D_2 f(x_1,z_2)\]
for some $z_1\in L(x_1,y_1)$, $z_2\in L(x_2,y_2)$.\qed


\begin{problembox}[12.21: Generalized Mean-Value Theorem]
State and prove a generalization of the result in Exercise 12.20 for a real-valued function differentiable on an \( n \)-ball \( B(x) \).
\end{problembox}

\noindent\textbf{Strategy:} Generalize the approach from Exercise 12.20 by constructing a function that varies one coordinate at a time. Apply the one-variable Mean Value Theorem to this function and use the chain rule to express the result in terms of partial derivatives.

\bigskip\noindent\textbf{Solution:}
For $f$ differentiable on a convex $B(x)$ and $y\in B(x)$, there exist $\xi_k\in L(x_k,y_k)$ such that
\[f(y)-f(x)=\sum_{k=1}^n (y_k-x_k)\,D_k f(x_1,\dots,\xi_k,\dots,y_n).\]
Proof: define $g(t)=\sum_{k=1}^n f(x_1,\dots,tx_k+(1-t)y_k,\dots,y_n)$ and apply the 1D MVT as in 12.20.\qed


\begin{problembox}[12.22: Mean-Value Theorem for Directional Derivatives]
Let \( f \) be real-valued and assume that the directional derivative \( f'(c + tu; u) \) exists for each \( t \) in the interval \( 0 \leq t \leq 1 \). Prove that for some \( \theta \) in the open interval \( (0, 1) \) we have
\[f(c + u) - f(c) = f'(c + \theta u; u).\]
\end{problembox}

\noindent\textbf{Strategy:} Define a one-variable function \( h(t) = f(c + tu) \) and apply the one-variable Mean Value Theorem to it on the interval \( [0,1] \). The derivative of \( h \) is the directional derivative of \( f \).

\bigskip\noindent\textbf{Solution:}
Apply the 1D MVT to $h(t)=f(c+tu)$ on $[0,1]$: $f(c+u)-f(c)=h'(\theta)=f'(c+\theta u;u)$ for some $\theta\in(0,1)$.\qed


\begin{problembox}[12.23: Zero Directional Derivatives]
\begin{enumerate}[label=(\alph*)]
\item If \( f \) is real-valued and if the directional derivative \( f'(x; u) = 0 \) for every \( x \) in an \( n \)-ball \( B(c) \) and every direction \( u \), prove that \( f \) is constant on \( B(c) \).
\item What can you conclude about \( f \) if \( f'(x; u) = 0 \) for a fixed direction \( u \) and every \( x \) in \( B(c) \)?
\end{enumerate}
\end{problembox}

\noindent\textbf{Strategy:} For part (a), use the fact that zero directional derivatives in all directions imply zero gradient, then integrate along paths to show constancy. For part (b), consider what happens when moving only in the direction \( u \).

\bigskip\noindent\textbf{Solution:}
\begin{enumerate}[label=(\alph*)]
\item If $f'(x;u)=0$ for all $u$, then $\nabla f(x)=0$ wherever $f$ is differentiable; integrating $\nabla f$ along any path in $B(c)$ gives $f\equiv\text{constant}$.
\item $f$ is constant along every line parallel to $u$; that is, $t\mapsto f(x+tu)$ is constant for each $x$.
\end{enumerate}\qed
\section{Derivatives of Higher Order and Taylor's Formula}

\noindent\textbf{Tools for this section.} Higher-order partials and multi-index notation; Clairaut/Schwarz theorem on equality of mixed partials under continuity; Taylor's formula with remainder in several variables; binomial coefficients for reorganizing terms when $n=2$.



\begin{problembox}[12.24: Equality of Mixed Partials]
For each of the following functions, verify that the mixed partial derivatives \( D_{1,2}f \) and \( D_{2,1}f \) are equal.
\begin{enumerate}[label=(\alph*)]
\item \( f(x, y) = x^4 + y^4 - 4x^2y^2 \).
\item \( f(x, y) = \log (x^2 + y^2) \), \( (x, y) \neq (0, 0) \).
\item \( f(x, y) = \tan (x^2/y) \), \( y \neq 0 \).
\end{enumerate}
\end{problembox}

\noindent\textbf{Strategy:} Use Clairaut's theorem which states that if the mixed partial derivatives are continuous, then they are equal. Alternatively, compute both mixed partials directly and verify they are identical.

\bigskip\noindent\textbf{Solution:}
All listed functions are $C^2$ on their stated domains; by Clairaut's theorem, $D_{1,2}f=D_{2,1}f$ there. Direct computation also confirms the equality.\qed


\begin{problembox}[12.25: Equality of Higher-Order Mixed Partials]
Let \( f \) be a function of two variables. Use induction and Theorem 12.13 to prove that if the \( 2^k \) partial derivatives of \( f \) of order \( k \) are continuous in a neighborhood of a point \( (x, y) \), then all mixed partials of the form \( D_{r_1, \ldots, r_k} f \) and \( D_{p_1, \ldots, p_k} f \) will be equal at \( (x, y) \) if the \( k \)-tuple \( (r_1, \ldots, r_k) \) contains the same number of ones as the \( k \)-tuple \( (p_1, \ldots, p_k) \).
\end{problembox}

\noindent\textbf{Strategy:} Use mathematical induction on the order \( k \). The base case \( k = 2 \) is Clairaut's theorem. For the inductive step, use the fact that any permutation can be achieved by swapping adjacent elements, and each swap is allowed by the \( k = 2 \) case.

\bigskip\noindent\textbf{Solution:}
For $k=2$ this is Clairaut's theorem. Assume the statement for order $k$. For order $k+1$, commute adjacent derivatives two at a time using the $k=2$ case (continuity ensures commutation), to reorder any arrangement into any other with the same number of 1's and 2's. Thus all mixed partials of the same type coincide.\qed


\begin{problembox}[12.26: Taylor's Formula for Two Variables]
If \( f \) is a function of two variables having continuous partials of order \( k \) on some open set \( S \) in \( \mathbb{R}^2 \), show that
\[f^{(k)} (x; t) = \sum_{r=0}^{k} \binom{k}{r} t_1^r t_2^{k-r} D_{p_1}, \ldots, p_k f(x), \quad \text{if } x \in S, \quad t = (t_1, t_2),\]
where in the \( r \)th term we have \( p_1 = \cdots = p_r = 1 \) and \( p_{r+1} = \cdots = p_k = 2 \). Use this result to give an alternative expression for Taylor's formula (Theorem 12.14) in the case when \( n = 2 \). The symbol \( \binom{k}{r} \) is the binomial coefficient \( k! / [r! (k - r)!] \).
\end{problembox}

\noindent\textbf{Strategy:} Use the multilinearity of the \( k \)th derivative and the fact that mixed partials of the same type are equal. The binomial coefficients arise from the number of ways to choose \( r \) derivatives with respect to the first variable and \( k-r \) with respect to the second.

\bigskip\noindent\textbf{Solution:}
By multilinearity and symmetry of mixed partials,
\[f^{(k)}(x;t)=\sum_{r=0}^k \binom{k}{r} t_1^r t_2^{k-r} D_{\underbrace{1,\dots,1}_{r},\underbrace{2,\dots,2}_{k-r}} f(x).\]
Hence, for $h=(h_1,h_2)$,
\begin{align*}
f(x+h)&=\sum_{j=0}^k \frac{1}{j!}\,f^{(j)}(x;h)+R_{k+1}(x,h)\\
&=\sum_{j=0}^k\sum_{r=0}^j \frac{1}{r!(j-r)!} D_{\!\underbrace{1,\dots,1}_{r},\underbrace{2,\dots,2}_{j-r}} f(x)\,h_1^{\,r} h_2^{\,j-r}+R_{k+1},
\end{align*}
which is the two-variable Taylor polynomial written by grouping powers of $h_1,h_2$.\qed


\begin{problembox}[12.27: Taylor Expansion]
Use Taylor's formula to express the following in powers of \( (x - 1) \) and \( (y - 2) \):
\begin{enumerate}[label=(\alph*)]
\item \( f(x, y) = x^3 + y^3 + xy^2 \),
\item \( f(x, y) = x^2 + xy + y^2 \).
\end{enumerate}
\end{problembox}

\noindent\textbf{Strategy:} Make the substitution \( \alpha = x - 1 \) and \( \beta = y - 2 \), then expand each term using the binomial theorem. Collect terms by powers of \( \alpha \) and \( \beta \).

\bigskip\noindent\textbf{Solution:}
Let $\alpha=x-1$, $\beta=y-2$.
\begin{enumerate}[label=(\alph*)]
\item $(1+\alpha)^3+(2+\beta)^3+(1+\alpha)(2+\beta)^2=13+7\alpha+16\beta+3\alpha^2+4\alpha\beta+7\beta^2+\alpha^3+\beta^3+\alpha\beta^2$.
\item $(1+\alpha)^2+(1+\alpha)(2+\beta)+(2+\beta)^2=7+4\alpha+5\beta+\alpha^2+\alpha\beta+\beta^2$.
\end{enumerate}