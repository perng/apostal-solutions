\chapter{Derivatives}

\section{Real-valued functions}

\subsection*{Definitions and Theorems}

\begin{definition}[Derivative]
The derivative of a function $f$ at a point $c$ is defined as
\[ f'(c) = \lim_{x \to c} \frac{f(x) - f(c)}{x - c} = \lim_{h \to 0} \frac{f(c + h) - f(c)}{h} \]
if this limit exists.
\end{definition}

\noindent\textbf{Importance:} The derivative is the fundamental concept of differential calculus, measuring the instantaneous rate of change of a function. It provides the slope of the tangent line and is essential for understanding how functions behave locally. This concept forms the foundation for optimization, motion analysis, and modeling dynamic systems.



\begin{definition}[Differentiable Function]
A function $f$ is differentiable at a point $c$ if $f'(c)$ exists. A function is differentiable on an interval if it is differentiable at every point in that interval.
\end{definition}

\noindent\textbf{Importance:} Differentiability is a stronger condition than continuity and ensures that the function has a well-defined linear approximation at each point. This property is crucial for applying calculus techniques and understanding the local behavior of functions.



\begin{definition}[Lipschitz Condition]
A function $f$ satisfies a Lipschitz condition of order $\alpha$ at $c$ if there exists a positive number $M$ and a neighborhood $B(c)$ of $c$ such that
\[ |f(x) - f(c)| \leq M|x - c|^\alpha \]
whenever $x \in B(c)$.
\end{definition}

\noindent\textbf{Importance:} The Lipschitz condition provides a quantitative measure of how rapidly a function can change. It ensures uniform continuity and is essential for proving existence and uniqueness of solutions to differential equations.



\begin{theorem}[Differentiability Implies Continuity]
If a function $f$ is differentiable at a point $c$, then $f$ is continuous at $c$.
\end{theorem}

\noindent\textbf{Importance:} This theorem establishes the hierarchy of smoothness properties: differentiability is stronger than continuity. It's fundamental for understanding the relationship between these two key concepts in calculus.



\begin{theorem}[Basic Differentiation Rules]
For differentiable functions $f$ and $g$:
\begin{enumerate}
\item $(f + g)'(x) = f'(x) + g'(x)$ (sum rule)
\item $(fg)'(x) = f'(x)g(x) + f(x)g'(x)$ (product rule)
\item $\left(\frac{f}{g}\right)'(x) = \frac{f'(x)g(x) - f(x)g'(x)}{g(x)^2}$ (quotient rule)
\item $(f \circ g)'(x) = f'(g(x))g'(x)$ (chain rule)
\end{enumerate}
\end{theorem}

\noindent\textbf{Importance:} These rules provide the computational foundation for finding derivatives of complex functions by breaking them down into simpler components. They are essential tools for all applications of calculus.



\begin{theorem}[Leibniz's Formula]
For functions $f$ and $g$ with $n$th derivatives, the $n$th derivative of their product is
\[ (fg)^{(n)}(x) = \sum_{k=0}^{n} \binom{n}{k} f^{(k)}(x)g^{(n-k)}(x) \]
\end{theorem}

\noindent\textbf{Importance:} This formula generalizes the product rule to higher derivatives and is essential for working with power series, differential equations, and advanced calculus problems involving repeated differentiation.



\begin{theorem}[Schwarzian Derivative]
The Schwarzian derivative of a function $f$ is defined as
\[ Sf(x) = \frac{f'''(x)}{f'(x)} - \frac{3}{2}\left(\frac{f''(x)}{f'(x)}\right)^2 \]
\end{theorem}

\noindent\textbf{Importance:} The Schwarzian derivative is invariant under Möbius transformations and plays a crucial role in complex analysis, differential geometry, and the study of conformal mappings. It measures how far a function is from being a Möbius transformation.



\begin{theorem}[Wronskian]
For functions $f_1, \ldots, f_n$ with $(n-1)$th derivatives, the Wronskian is the determinant
\[ W(f_1, \ldots, f_n)(x) = \begin{vmatrix}
f_1(x) & f_2(x) & \cdots & f_n(x) \\
f_1'(x) & f_2'(x) & \cdots & f_n'(x) \\
\vdots & \vdots & \ddots & \vdots \\
f_1^{(n-1)}(x) & f_2^{(n-1)}(x) & \cdots & f_n^{(n-1)}(x)
\end{vmatrix} \]
\end{theorem}

\noindent\textbf{Importance:} The Wronskian is a powerful tool for determining linear independence of solutions to differential equations. It provides a systematic way to check whether a set of functions forms a fundamental solution set.



\begin{theorem}[Rolle's Theorem]
If $f$ is continuous on $[a,b]$, differentiable on $(a,b)$, and $f(a) = f(b)$, then there exists $c \in (a,b)$ such that $f'(c) = 0$.
\end{theorem}

\noindent\textbf{Importance:} Rolle's theorem is the foundation for the Mean Value Theorem and provides a geometric interpretation of the relationship between function values and derivatives. It's essential for proving existence of critical points and understanding function behavior.



\begin{theorem}[Mean Value Theorem]
If $f$ is continuous on $[a,b]$ and differentiable on $(a,b)$, then there exists $c \in (a,b)$ such that
\[ f'(c) = \frac{f(b) - f(a)}{b - a} \]
\end{theorem}

\noindent\textbf{Importance:} The Mean Value Theorem is one of the most important results in calculus, connecting average rates of change with instantaneous rates of change. It's fundamental for proving many other theorems and understanding function behavior.



\begin{theorem}[Cauchy's Mean Value Theorem]
If $f$ and $g$ are continuous on $[a,b]$ and differentiable on $(a,b)$, and $g'(x) \neq 0$ for all $x \in (a,b)$, then there exists $c \in (a,b)$ such that
\[ \frac{f'(c)}{g'(c)} = \frac{f(b) - f(a)}{g(b) - g(a)} \]
\end{theorem}

\noindent\textbf{Importance:} This generalization of the Mean Value Theorem is essential for proving L'Hôpital's rule and understanding the relationship between different functions' rates of change. It's crucial for advanced calculus techniques.



\begin{theorem}[Generalized Mean Value Theorem]
If $f$ has a finite $n$th derivative in $[a,b]$ and $f^{(k)}(a) = 0$ for $k = 0, 1, \ldots, n-1$, then there exists $c \in (a,b)$ such that
\[ f^{(n)}(c) = \frac{n!}{(b-a)^n} f(b) \]
\end{theorem}

\noindent\textbf{Importance:} This theorem provides a powerful tool for understanding higher-order derivatives and is essential for Taylor series theory. It connects function values with derivatives of arbitrary order.



\begin{theorem}[Taylor's Theorem with Lagrange Remainder]
If $f$ has a finite $(n+1)$th derivative in $[a,b]$, then for any $x \in [a,b]$,
\[ f(x) = \sum_{k=0}^{n} \frac{f^{(k)}(a)}{k!} (x-a)^k + \frac{f^{(n+1)}(c)}{(n+1)!} (x-a)^{n+1} \]
where $c$ is some point between $a$ and $x$.
\end{theorem}

\noindent\textbf{Importance:} Taylor's theorem is fundamental for approximating functions by polynomials and understanding the relationship between a function and its derivatives. It's essential for numerical analysis and approximation theory.



\begin{theorem}[Taylor's Theorem with Cauchy Remainder]
If $f$ has a finite $(n+1)$th derivative in $[a,b]$, then for any $x \in [a,b]$,
\[ f(x) = \sum_{k=0}^{n} \frac{f^{(k)}(a)}{k!} (x-a)^k + \frac{(x-a)(x-c)^n}{n!} f^{(n+1)}(c) \]
where $c$ is some point between $a$ and $x$.
\end{theorem}

\noindent\textbf{Importance:} This alternative form of Taylor's theorem provides different error estimates and is useful in specific applications where the Cauchy remainder form is more convenient than the Lagrange form.



\begin{theorem}[L'Hôpital's Rule]
If $f$ and $g$ are differentiable on $(a,b)$ except possibly at $c \in (a,b)$, $\lim_{x \to c} f(x) = \lim_{x \to c} g(x) = 0$ or $\pm\infty$, and $\lim_{x \to c} \frac{f'(x)}{g'(x)}$ exists, then
\[ \lim_{x \to c} \frac{f(x)}{g(x)} = \lim_{x \to c} \frac{f'(x)}{g'(x)} \]
\end{theorem}

\noindent\textbf{Importance:} L'Hôpital's rule is an essential tool for evaluating limits of indeterminate forms. It provides a systematic method for resolving $0/0$ and $\infty/\infty$ limits that arise frequently in calculus and analysis.



\begin{definition}[Vector-Valued Function]
A vector-valued function is a function $f: \mathbb{R} \to \mathbb{R}^n$ that maps real numbers to vectors in $n$-dimensional space.
\end{definition}

\noindent\textbf{Importance:} Vector-valued functions are essential for modeling multi-dimensional phenomena, such as motion in space, forces in physics, and complex systems. They extend calculus concepts to higher dimensions.



\begin{definition}[Vector-Valued Derivative]
A vector-valued function $f$ is differentiable at $c$ if there exists a vector $f'(c)$ such that
\[ \lim_{h \to 0} \frac{\|f(c + h) - f(c) - f'(c)h\|}{|h|} = 0 \]
\end{definition}

\noindent\textbf{Importance:} This definition extends the concept of derivative to vector-valued functions, providing a framework for analyzing rates of change in multi-dimensional spaces. It's fundamental for vector calculus and differential geometry.



\begin{theorem}[Component-wise Differentiation]
A vector-valued function $f = (f_1, f_2, \ldots, f_n)$ is differentiable at $c$ if and only if each component function $f_i$ is differentiable at $c$, and
\[ f'(c) = (f_1'(c), f_2'(c), \ldots, f_n'(c)) \]
\end{theorem}

\noindent\textbf{Importance:} This theorem reduces the study of vector-valued functions to the study of their scalar components, making vector calculus more accessible and computable. It's fundamental for practical applications.



\begin{theorem}[Dot Product Rule]
If $f$ and $g$ are differentiable vector-valued functions, then
\[ \frac{d}{dt}(f(t) \cdot g(t)) = f'(t) \cdot g(t) + f(t) \cdot g'(t) \]
\end{theorem}

\noindent\textbf{Importance:} This rule extends the product rule to vector-valued functions and is essential for analyzing physical quantities like work, energy, and angular momentum. It's fundamental for vector calculus applications.



\begin{theorem}[Constant Norm Implies Orthogonality]
If a vector-valued function $f$ has constant norm $\|f(t)\| = C$ for all $t$, then $f(t) \cdot f'(t) = 0$ for all $t$.
\end{theorem}

\noindent\textbf{Importance:} This theorem provides a geometric interpretation of vector-valued derivatives and is essential for understanding motion along curves, particularly circular motion and other constrained trajectories.



\begin{definition}[Partial Derivative]
The partial derivative of a function $f(x_1, x_2, \ldots, x_n)$ with respect to $x_i$ at a point $(a_1, a_2, \ldots, a_n)$ is defined as
\[ \frac{\partial f}{\partial x_i}(a_1, \ldots, a_n) = \lim_{h \to 0} \frac{f(a_1, \ldots, a_i + h, \ldots, a_n) - f(a_1, \ldots, a_n)}{h} \]
if this limit exists.
\end{definition}

\noindent\textbf{Importance:} Partial derivatives are fundamental for studying functions of several variables and are essential for optimization, differential equations, and modeling multi-dimensional phenomena. They measure how a function changes with respect to one variable while holding others constant.



\begin{definition}[Directional Derivative]
The directional derivative of $f$ at a point $a$ in the direction of a unit vector $u$ is defined as
\[ D_u f(a) = \lim_{h \to 0} \frac{f(a + hu) - f(a)}{h} \]
if this limit exists.
\end{definition}

\noindent\textbf{Importance:} Directional derivatives measure how a function changes in any specified direction, not just along coordinate axes. They are essential for understanding function behavior in multi-dimensional spaces and for optimization algorithms.



\begin{definition}[Gradient]
The gradient of a function $f$ at a point $a$ is the vector of partial derivatives:
\[ \nabla f(a) = \left(\frac{\partial f}{\partial x_1}(a), \frac{\partial f}{\partial x_2}(a), \ldots, \frac{\partial f}{\partial x_n}(a)\right) \]
\end{definition}

\noindent\textbf{Importance:} The gradient points in the direction of steepest ascent and its magnitude gives the rate of change in that direction. It's fundamental for optimization, differential equations, and understanding function behavior in multi-dimensional spaces.



\begin{theorem}[Mixed Partial Derivatives]
If the mixed partial derivatives $\frac{\partial^2 f}{\partial x \partial y}$ and $\frac{\partial^2 f}{\partial y \partial x}$ are continuous in a neighborhood of a point, then they are equal at that point.
\end{theorem}

\noindent\textbf{Importance:} This theorem (Clairaut's theorem) ensures that the order of differentiation doesn't matter for mixed partial derivatives under appropriate conditions. It's essential for simplifying calculations and proving results in multivariable calculus.



\begin{theorem}[Directional Derivative and Gradient]
If $f$ is differentiable at $a$, then the directional derivative in the direction of unit vector $u$ is
\[ D_u f(a) = \nabla f(a) \cdot u \]
\end{theorem}

\noindent\textbf{Importance:} This theorem connects directional derivatives with the gradient, providing a powerful computational tool and geometric interpretation. It's fundamental for optimization algorithms and understanding function behavior.



\begin{theorem}[Chain Rule for Partial Derivatives]
If $f(x,y)$ is differentiable and $x = x(t)$, $y = y(t)$ are differentiable functions, then
\[ \frac{d}{dt}f(x(t), y(t)) = \frac{\partial f}{\partial x}\frac{dx}{dt} + \frac{\partial f}{\partial y}\frac{dy}{dt} \]
\end{theorem}

\noindent\textbf{Importance:} This theorem extends the chain rule to functions of several variables and is essential for computing derivatives of composite functions. It's fundamental for differential equations and optimization problems.



\begin{definition}[Complex Derivative]
A function $f: \mathbb{C} \to \mathbb{C}$ is differentiable at a point $z_0$ if the limit
\[ f'(z_0) = \lim_{z \to z_0} \frac{f(z) - f(z_0)}{z - z_0} \]
exists.
\end{definition}

\noindent\textbf{Importance:} Complex differentiability is a much stronger condition than real differentiability and leads to remarkable properties like analyticity and the Cauchy integral formula. It's fundamental for complex analysis and its applications.



\begin{definition}[Holomorphic Function]
A function $f$ is holomorphic (analytic) on an open set $U \subseteq \mathbb{C}$ if it is differentiable at every point in $U$.
\end{definition}

\noindent\textbf{Importance:} Holomorphic functions have extraordinary properties like infinite differentiability, power series representations, and the maximum principle. They are central to complex analysis and have wide applications in physics and engineering.



\begin{definition}[Cauchy-Riemann Equations]
For a complex function $f(z) = u(x,y) + iv(x,y)$, the Cauchy-Riemann equations are
\[ \frac{\partial u}{\partial x} = \frac{\partial v}{\partial y} \quad \text{and} \quad \frac{\partial u}{\partial y} = -\frac{\partial v}{\partial x} \]
\end{definition}

\noindent\textbf{Importance:} The Cauchy-Riemann equations provide a necessary and sufficient condition for complex differentiability in terms of real partial derivatives. They connect complex analysis with multivariable calculus and are essential for understanding complex functions.



\begin{theorem}[Cauchy-Riemann Criterion]
A complex function $f = u + iv$ is differentiable at a point if and only if the partial derivatives of $u$ and $v$ exist, are continuous, and satisfy the Cauchy-Riemann equations at that point.
\end{theorem}

\noindent\textbf{Importance:} This theorem provides a practical test for complex differentiability and connects complex analysis with real analysis. It's fundamental for proving results in complex analysis and understanding the relationship between real and complex functions.



\begin{theorem}[Complex Chain Rule]
If $f$ and $g$ are differentiable complex functions, then
\[ (f \circ g)'(z) = f'(g(z))g'(z) \]
\end{theorem}

\noindent\textbf{Importance:} This theorem extends the chain rule to complex functions and is essential for computing derivatives of composite complex functions. It's fundamental for complex analysis and its applications.



\begin{theorem}[Complex Product Rule]
If $f$ and $g$ are differentiable complex functions, then
\[ (fg)'(z) = f'(z)g(z) + f(z)g'(z) \]
\end{theorem}

\noindent\textbf{Importance:} This theorem extends the product rule to complex functions and is essential for computing derivatives of products of complex functions. It's fundamental for complex analysis and its applications.



\begin{theorem}[Complex Quotient Rule]
If $f$ and $g$ are differentiable complex functions and $g(z) \neq 0$, then
\[ \left(\frac{f}{g}\right)'(z) = \frac{f'(z)g(z) - f(z)g'(z)}{g(z)^2} \]
\end{theorem}

\noindent\textbf{Importance:} This theorem extends the quotient rule to complex functions and is essential for computing derivatives of quotients of complex functions. It's fundamental for complex analysis and its applications.



\begin{theorem}[Identity Principle]
If two holomorphic functions $f$ and $g$ agree on a set with a limit point in their common domain, then $f = g$ throughout their common domain.
\end{theorem}

\noindent\textbf{Importance:} The identity principle is a remarkable property of holomorphic functions that has no analogue in real analysis. It shows that holomorphic functions are completely determined by their values on very small sets, making them extremely rigid.



