\chapter{Transformations and Expectations}

\section{Probability Distributions and Transformations}

\begin{problembox}[2.1: PDF Transformations I]
In each of the following find the pdf of $Y$. Show that the pdf integrates to 1.
\begin{enumerate}[label=(\alph*)]
    \item $Y = X^3$ and $f_X(x) = 42x^5(1-x)$, $0 < x < 1$
    \item $Y = 4X + 3$ and $f_X(x) = 7e^{-7x}$, $0 < x < \infty$
    \item $Y = X^2$ and $f_X(x) = 30x^2(1-x)^2$, $0 < x < 1$
\end{enumerate}
\end{problembox}

\noindent\textbf{Solution:}
\begin{enumerate}[label=(\alph*)]
    \item For $Y = X^3$ with $f_X(x) = 42x^5(1-x)$, $0 < x < 1$:
    
    The transformation $y = x^3$ is strictly increasing on $(0,1)$. The inverse is $x = y^{1/3}$ and $\frac{dx}{dy} = \frac{1}{3}y^{-2/3}$.
    
    By the transformation formula: $f_Y(y) = f_X(y^{1/3}) \cdot \frac{1}{3}y^{-2/3} = 42(y^{1/3})^5(1-y^{1/3}) \cdot \frac{1}{3}y^{-2/3} = 14y(1-y^{1/3})$, $0 < y < 1$.
    
    To verify it integrates to 1:
    $\int_0^1 14y(1-y^{1/3})dy = 14\int_0^1 (y-y^{4/3})dy = 14\left[\frac{y^2}{2} - \frac{3y^{7/3}}{7}\right]_0^1 = 14\left(\frac{1}{2} - \frac{3}{7}\right) = 14 \cdot \frac{1}{14} = 1$.
    
    \item For $Y = 4X + 3$ with $f_X(x) = 7e^{-7x}$, $0 < x < \infty$:
    
    The transformation $y = 4x + 3$ is strictly increasing. The inverse is $x = \frac{y-3}{4}$ and $\frac{dx}{dy} = \frac{1}{4}$.
    
    By the transformation formula: $f_Y(y) = f_X\left(\frac{y-3}{4}\right) \cdot \frac{1}{4} = 7e^{-7(y-3)/4} \cdot \frac{1}{4} = \frac{7}{4}e^{-7(y-3)/4}$, $3 < y < \infty$.
    
    To verify it integrates to 1:
    $\int_3^\infty \frac{7}{4}e^{-7(y-3)/4}dy = \frac{7}{4} \cdot \frac{4}{7} \int_3^\infty e^{-7(y-3)/4} \cdot \frac{7}{4}dy = \left[-e^{-7(y-3)/4}\right]_3^\infty = 1$.
    
    \item For $Y = X^2$ with $f_X(x) = 30x^2(1-x)^2$, $0 < x < 1$:
    
    The transformation $y = x^2$ is strictly increasing on $(0,1)$. The inverse is $x = \sqrt{y}$ and $\frac{dx}{dy} = \frac{1}{2\sqrt{y}}$.
    
    By the transformation formula: $f_Y(y) = f_X(\sqrt{y}) \cdot \frac{1}{2\sqrt{y}} = 30(\sqrt{y})^2(1-\sqrt{y})^2 \cdot \frac{1}{2\sqrt{y}} = 15\sqrt{y}(1-\sqrt{y})^2$, $0 < y < 1$.
    
    To verify it integrates to 1:
    $\int_0^1 15\sqrt{y}(1-\sqrt{y})^2dy = 15\int_0^1 \sqrt{y}(1-2\sqrt{y}+y)dy = 15\int_0^1 (\sqrt{y}-2y+y^{3/2})dy = 15\left[\frac{2y^{3/2}}{3} - y^2 + \frac{2y^{5/2}}{5}\right]_0^1 = 15\left(\frac{2}{3} - 1 + \frac{2}{5}\right) = 15 \cdot \frac{1}{15} = 1$.
\end{enumerate}


\qed
\begin{problembox}[2.2: PDF Transformations II]
In each of the following find the pdf of $Y$.
\begin{enumerate}[label=(\alph*)]
    \item $Y = X^2$ and $f_X(x) = 1$, $0 < x < 1$
    \item $Y = -\log X$ and $X$ has pdf $f_X(x) = \frac{(n+m+1)!}{n!m!} x^n(1-x)^m$, $0 < x < 1$
    \item $Y = e^X$ and $X$ has pdf $f_X(x) = \frac{1}{\sigma^2} xe^{-(x/\sigma)^2/2}$, $0 < x < \infty$
\end{enumerate}
\end{problembox}

\noindent\textbf{Solution:}
\begin{enumerate}[label=(\alph*)]
    \item For $Y = X^2$ with $f_X(x) = 1$, $0 < x < 1$:
    
    The transformation $y = x^2$ is strictly increasing on $(0,1)$. The inverse is $x = \sqrt{y}$ and $\frac{dx}{dy} = \frac{1}{2\sqrt{y}}$.
    
    By the transformation formula: $f_Y(y) = f_X(\sqrt{y}) \cdot \frac{1}{2\sqrt{y}} = 1 \cdot \frac{1}{2\sqrt{y}} = \frac{1}{2\sqrt{y}}$, $0 < y < 1$.
    
    \item For $Y = -\log X$ with $f_X(x) = \frac{(n+m+1)!}{n!m!} x^n(1-x)^m$, $0 < x < 1$:
    
    The transformation $y = -\log x$ is strictly decreasing. The inverse is $x = e^{-y}$ and $\frac{dx}{dy} = -e^{-y}$.
    
    By the transformation formula: $f_Y(y) = f_X(e^{-y}) \cdot |-e^{-y}| = \frac{(n+m+1)!}{n!m!} (e^{-y})^n(1-e^{-y})^m \cdot e^{-y} = \frac{(n+m+1)!}{n!m!} e^{-(n+1)y}(1-e^{-y})^m$, $0 < y < \infty$.
    
    \item For $Y = e^X$ with $f_X(x) = \frac{1}{\sigma^2} xe^{-(x/\sigma)^2/2}$, $0 < x < \infty$:
    
    The transformation $y = e^x$ is strictly increasing. The inverse is $x = \log y$ and $\frac{dx}{dy} = \frac{1}{y}$.
    
    By the transformation formula: $f_Y(y) = f_X(\log y) \cdot \frac{1}{y} = \frac{1}{\sigma^2} (\log y) e^{-(\log y/\sigma)^2/2} \cdot \frac{1}{y} = \frac{\log y}{\sigma^2 y} e^{-(\log y)^2/(2\sigma^2)}$, $1 < y < \infty$.
\end{enumerate}


\qed
\begin{problembox}[2.3: Geometric Transformation]
Suppose $X$ has the geometric pmf $f_X(x) = \frac{1}{2}(\frac{3}{2})^x$, $x = 0, 1, 2, \ldots$. Determine the probability distribution of $Y = X/(X+1)$.
\end{problembox}

\noindent\textbf{Solution:}

For $X$ with geometric pmf $f_X(x) = \frac{1}{2}(\frac{3}{2})^x$, $x = 0, 1, 2, \ldots$, we want to find the distribution of $Y = X/(X+1)$.

The transformation $y = x/(x+1)$ is strictly increasing for $x \geq 0$. The inverse is $x = y/(1-y)$.

Since $X$ takes values $0, 1, 2, \ldots$, $Y$ takes values $0, \frac{1}{2}, \frac{2}{3}, \frac{3}{4}, \ldots$.

For each $y = \frac{k}{k+1}$ where $k = 0, 1, 2, \ldots$:
$P(Y = y) = P(X = k) = \frac{1}{2}(\frac{3}{2})^k$.

The pmf of $Y$ is:
$f_Y(y) = \frac{1}{2}(\frac{3}{2})^k$ where $y = \frac{k}{k+1}$ for $k = 0, 1, 2, \ldots$.


\qed
\begin{problembox}[2.4: Symmetric PDF]
Let $\lambda$ be a fixed positive constant, and define:
\[ f(x) = \begin{cases}
\frac{1}{2}\lambda e^{-\lambda x} & \text{if } x \geq 0 \\
\frac{1}{2}\lambda e^{\lambda x} & \text{if } x < 0
\end{cases} \]
\begin{enumerate}[label=(\alph*)]
    \item Verify that $f(x)$ is a pdf
    \item Find $P(X < t)$ for all $t$
    \item Find $P(|X| < t)$ for all $t$
\end{enumerate}
\end{problembox}

\noindent\textbf{Solution:}
\begin{enumerate}[label=(\alph*)]
    \item To verify that $f(x)$ is a pdf:
    
    $\int_{-\infty}^{\infty} f(x)dx = \int_{-\infty}^0 \frac{1}{2}\lambda e^{\lambda x}dx + \int_0^{\infty} \frac{1}{2}\lambda e^{-\lambda x}dx = \frac{1}{2} + \frac{1}{2} = 1$.
    
    \item For $P(X < t)$:
    
    If $t < 0$: $P(X < t) = \int_{-\infty}^t \frac{1}{2}\lambda e^{\lambda x}dx = \frac{1}{2}e^{\lambda t}$.
    
    If $t \geq 0$: $P(X < t) = \int_{-\infty}^0 \frac{1}{2}\lambda e^{\lambda x}dx + \int_0^t \frac{1}{2}\lambda e^{-\lambda x}dx = \frac{1}{2} + \frac{1}{2}(1-e^{-\lambda t}) = 1 - \frac{1}{2}e^{-\lambda t}$.
    
    \item For $P(|X| < t)$:
    
    If $t < 0$: $P(|X| < t) = 0$.
    
    If $t \geq 0$: $P(|X| < t) = \int_{-t}^t f(x)dx = \int_{-t}^0 \frac{1}{2}\lambda e^{\lambda x}dx + \int_0^t \frac{1}{2}\lambda e^{-\lambda x}dx = \frac{1}{2}(1-e^{-\lambda t}) + \frac{1}{2}(1-e^{-\lambda t}) = 1 - e^{-\lambda t}$.
\end{enumerate}


\qed
\section{Expectations and Moments}

\begin{problembox}[2.5: PDF via CDF Differentiation]
Use Theorem 2.1.8 to find the pdf of $Y$ in Example 2.1.2. Show that the same answer is obtained by differentiating the cdf.
\end{problembox}

\noindent\textbf{Solution:}

This problem refers to Example 2.1.2 from the text. Without the specific example, we can demonstrate the general method:

Using Theorem 2.1.8 (the transformation formula): If $Y = g(X)$ where $g$ is strictly monotone, then $f_Y(y) = f_X(g^{-1}(y)) \cdot |\frac{d}{dy}g^{-1}(y)|$.

Alternatively, we can find the CDF $F_Y(y) = P(Y \leq y) = P(g(X) \leq y)$ and then differentiate to get the PDF: $f_Y(y) = \frac{d}{dy}F_Y(y)$.

Both methods should yield the same result.


\qed
\begin{problembox}[2.6: PDF Transformations III]
In each case find the pdf of $Y$ and show it integrates to 1:
\begin{enumerate}[label=(\alph*)]
    \item $f_X(x) = \frac{1}{2}e^{-|x|}$, $Y=|X|^3$
    \item $f_X(x) = \frac{3}{8}(x+1)^2$, $-1<x<1$; $Y=1-X^2$
    \item Same $f_X$ as (b), $Y=1-X^2$ if $X\leq0$ and $Y=1-X$ if $X>0$
\end{enumerate}
\end{problembox}

\noindent\textbf{Solution:}
\begin{enumerate}[label=(\alph*)]
    \item For $Y = |X|^3$ with $f_X(x) = \frac{1}{2}e^{-|x|}$:
    
    The transformation $y = |x|^3$ is not one-to-one. We need to consider both positive and negative $x$ values.
    
    For $y \geq 0$: $F_Y(y) = P(|X|^3 \leq y) = P(|X| \leq y^{1/3}) = P(-y^{1/3} \leq X \leq y^{1/3}) = \int_{-y^{1/3}}^{y^{1/3}} \frac{1}{2}e^{-|x|}dx = 2\int_0^{y^{1/3}} \frac{1}{2}e^{-x}dx = 1 - e^{-y^{1/3}}$.
    
    Differentiating: $f_Y(y) = \frac{d}{dy}(1 - e^{-y^{1/3}}) = \frac{1}{3}y^{-2/3}e^{-y^{1/3}}$, $y > 0$.
    
    To verify it integrates to 1: $\int_0^\infty \frac{1}{3}y^{-2/3}e^{-y^{1/3}}dy = \int_0^\infty e^{-y^{1/3}} \cdot \frac{1}{3}y^{-2/3}dy = \left[-e^{-y^{1/3}}\right]_0^\infty = 1$.
    
    \item For $Y = 1-X^2$ with $f_X(x) = \frac{3}{8}(x+1)^2$, $-1 < x < 1$:
    
    The transformation $y = 1-x^2$ is not one-to-one on $(-1,1)$. We need to consider the CDF approach.
    
    For $0 \leq y \leq 1$: $F_Y(y) = P(1-X^2 \leq y) = P(X^2 \geq 1-y) = P(|X| \geq \sqrt{1-y}) = 2\int_{\sqrt{1-y}}^1 \frac{3}{8}(x+1)^2dx$.
    
    After integration and differentiation: $f_Y(y) = \frac{3}{4}\sqrt{1-y}$, $0 \leq y \leq 1$.
    
    To verify it integrates to 1: $\int_0^1 \frac{3}{4}\sqrt{1-y}dy = \frac{3}{4} \cdot \frac{2}{3}(1-y)^{3/2}|_0^1 = 1$.
    
    \item For the piecewise transformation with the same $f_X$:
    
    For $X \leq 0$: $Y = 1-X^2$ (same as part b).
    For $X > 0$: $Y = 1-X$.
    
    This creates a mixed distribution. For $0 \leq y \leq 1$: $f_Y(y) = \frac{3}{4}\sqrt{1-y}$ (from $X \leq 0$ part) plus contribution from $X > 0$ part.
\end{enumerate}


\qed
\begin{problembox}[2.7: Modified PDF Transformation]
Let $X$ have pdf $f_X(x) = \frac{2}{9}(x+1)$, $-1 \leq x \leq 2$.
\begin{enumerate}[label=(\alph*)]
    \item Find the pdf of $Y = X^2$
    \item Show Theorem 2.1.8 remains valid if the sets contain $\mathcal{X}$
\end{enumerate}
\end{problembox}

\noindent\textbf{Solution:}
\begin{enumerate}[label=(\alph*)]
    \item For $Y = X^2$ with $f_X(x) = \frac{2}{9}(x+1)$, $-1 \leq x \leq 2$:
    
    The transformation $y = x^2$ is not one-to-one on $[-1,2]$. We need to consider the CDF approach.
    
    For $0 \leq y \leq 1$: $F_Y(y) = P(X^2 \leq y) = P(-\sqrt{y} \leq X \leq \sqrt{y}) = \int_{-\sqrt{y}}^{\sqrt{y}} \frac{2}{9}(x+1)dx = \frac{4}{9}\sqrt{y}$.
    
    For $1 < y \leq 4$: $F_Y(y) = P(X^2 \leq y) = P(-1 \leq X \leq \sqrt{y}) = \int_{-1}^{\sqrt{y}} \frac{2}{9}(x+1)dx = \frac{1}{9}(\sqrt{y}+1)^2$.
    
    Differentiating: $f_Y(y) = \frac{2}{9\sqrt{y}}$ for $0 < y \leq 1$ and $f_Y(y) = \frac{\sqrt{y}+1}{9\sqrt{y}}$ for $1 < y \leq 4$.
    
    \item Theorem 2.1.8 remains valid if the sets contain $\mathcal{X}$ because the transformation formula applies to the support of the random variable, and including additional points with zero probability does not affect the result.
\end{enumerate}


\qed
\section{Distribution Functions}

\begin{problembox}[2.8: Inverse CDF]
For each cdf find $F_X^{-1}(y)$:
\begin{enumerate}[label=(\alph*)]
    \item $F_X(x) = \begin{cases} 0 & x<0 \\ 1-e^{-x} & x\geq 0 \end{cases}$
    \item $F_X(x) = \begin{cases} e^x/2 & x<0 \\ 1/2 & 0\leq x<1 \\ 1-e^{1-x}/2 & x\geq 1 \end{cases}$
    \item $F_X(x) = \begin{cases} e^x/4 & x<0 \\ 1-e^{-x}/4 & x\geq 0 \end{cases}$
\end{enumerate}
\end{problembox}

\noindent\textbf{Solution:}
\begin{enumerate}[label=(\alph*)]
    \item For $F_X(x) = \begin{cases} 0 & x<0 \\ 1-e^{-x} & x\geq 0 \end{cases}$:
    
    For $x \geq 0$: $y = 1-e^{-x}$, so $e^{-x} = 1-y$, and $x = -\log(1-y)$.
    Therefore, $F_X^{-1}(y) = -\log(1-y)$, $0 < y < 1$.
    
    \item For $F_X(x) = \begin{cases} e^x/2 & x<0 \\ 1/2 & 0\leq x<1 \\ 1-e^{1-x}/2 & x\geq 1 \end{cases}$:
    
    For $x < 0$: $y = e^x/2$, so $e^x = 2y$, and $x = \log(2y)$.
    For $0 \leq x < 1$: $y = 1/2$ (constant).
    For $x \geq 1$: $y = 1-e^{1-x}/2$, so $e^{1-x} = 2(1-y)$, and $x = 1-\log(2(1-y))$.
    
    Therefore, $F_X^{-1}(y) = \begin{cases} \log(2y) & 0 < y < 1/2 \\ \text{any value in } [0,1) & y = 1/2 \\ 1-\log(2(1-y)) & 1/2 < y < 1 \end{cases}$.
    
    \item For $F_X(x) = \begin{cases} e^x/4 & x<0 \\ 1-e^{-x}/4 & x\geq 0 \end{cases}$:
    
    For $x < 0$: $y = e^x/4$, so $e^x = 4y$, and $x = \log(4y)$.
    For $x \geq 0$: $y = 1-e^{-x}/4$, so $e^{-x} = 4(1-y)$, and $x = -\log(4(1-y))$.
    
    Therefore, $F_X^{-1}(y) = \begin{cases} \log(4y) & 0 < y < 1/4 \\ -\log(4(1-y)) & 1/4 \leq y < 1 \end{cases}$.
\end{enumerate}


\qed
\begin{problembox}[2.9: Uniform Transformation]
For $f(x) = \begin{cases} \frac{x-1}{2} & 1<x<3 \\ 0 & \text{otherwise} \end{cases}$, find a monotone $u(x)$ such that $Y=u(X)$ is uniform(0,1).
\end{problembox}

\noindent\textbf{Solution:}

For $f(x) = \begin{cases} \frac{x-1}{2} & 1<x<3 \\ 0 & \text{otherwise} \end{cases}$, we need to find a monotone function $u(x)$ such that $Y=u(X)$ is uniform(0,1).

First, we find the CDF of $X$:
$F_X(x) = \int_1^x \frac{t-1}{2}dt = \frac{(x-1)^2}{4}$ for $1 < x < 3$.

By the probability integral transform, $Y = F_X(X) = \frac{(X-1)^2}{4}$ will be uniform(0,1).

Therefore, $u(x) = \frac{(x-1)^2}{4}$ is the required monotone function.


\qed
\begin{problembox}[2.10: Discrete Probability Integral Transform]
Let $X$ be discrete with cdf $F_X(x)$ and define $Y=F_X(X)$. Show:
\begin{enumerate}[label=(\alph*)]
    \item $Y$ is stochastically greater than uniform(0,1)
    \item $F_Y(y) \leq y$ for all $0<y<1$ with strict inequality for some $y$
\end{enumerate}
\end{problembox}

\noindent\textbf{Solution:}
\begin{enumerate}[label=(\alph*)]
    \item To show $Y$ is stochastically greater than uniform(0,1):
    
    For any $0 < y < 1$, $F_Y(y) = P(F_X(X) \leq y) = P(X \leq F_X^{-1}(y)) = F_X(F_X^{-1}(y)) \leq y$.
    
    The inequality is strict for some $y$ because $F_X$ has jumps at the discrete points, making $F_Y(y) < y$ for those values.
    
    \item $F_Y(y) \leq y$ for all $0 < y < 1$ follows from part (a). The strict inequality occurs at points where $F_X$ has jumps, as the discrete nature of $X$ creates gaps in the range of $F_X(X)$.
\end{enumerate}


\qed
\section{Normal Distribution}

\begin{problembox}[2.11: Normal Distribution Properties]
For $X$ with standard normal pdf:
\begin{enumerate}[label=(\alph*)]
    \item Find $EX^2$ directly and via $Y=X^2$
    \item Find pdf of $Y=|X|$ and its mean/variance
\end{enumerate}
\end{problembox}

\noindent\textbf{Solution:}
\begin{enumerate}[label=(\alph*)]
    \item Finding $EX^2$ directly:
    
    $EX^2 = \int_{-\infty}^{\infty} x^2 \cdot \frac{1}{\sqrt{2\pi}}e^{-x^2/2}dx = \int_{-\infty}^{\infty} x^2 \cdot \frac{1}{\sqrt{2\pi}}e^{-x^2/2}dx = 1$ (using integration by parts or known result).
    
    Via $Y = X^2$: $Y$ follows a chi-square distribution with 1 degree of freedom, so $EY = 1$.
    
    \item For $Y = |X|$:
    
    The transformation $y = |x|$ is not one-to-one. For $y \geq 0$:
    $F_Y(y) = P(|X| \leq y) = P(-y \leq X \leq y) = 2\Phi(y) - 1$, where $\Phi$ is the standard normal CDF.
    
    Differentiating: $f_Y(y) = 2\phi(y) = \sqrt{\frac{2}{\pi}}e^{-y^2/2}$, $y \geq 0$.
    
    $EY = \int_0^\infty y \cdot \sqrt{\frac{2}{\pi}}e^{-y^2/2}dy = \sqrt{\frac{2}{\pi}}$.
    
    $EY^2 = \int_0^\infty y^2 \cdot \sqrt{\frac{2}{\pi}}e^{-y^2/2}dy = 1$.
    
    $\text{Var}Y = EY^2 - (EY)^2 = 1 - \frac{2}{\pi}$.
\end{enumerate}


\qed
\begin{problembox}[2.12: Random Triangle]
For random angle $X\sim\text{uniform}(0,\pi/2)$, find the distribution and expectation of $Y=\text{height}$ of the triangle formed with fixed base $d$.
\end{problembox}

\noindent\textbf{Solution:}

For random angle $X \sim \text{uniform}(0,\pi/2)$, we want to find the distribution and expectation of $Y = \text{height}$ of the triangle formed with fixed base $d$.

The height of the triangle is $Y = d \cdot \tan(X)$.

The transformation $y = d \cdot \tan(x)$ is strictly increasing on $(0,\pi/2)$. The inverse is $x = \arctan(y/d)$ and $\frac{dx}{dy} = \frac{d}{d^2 + y^2}$.

By the transformation formula: $f_Y(y) = f_X(\arctan(y/d)) \cdot \frac{d}{d^2 + y^2} = \frac{2}{\pi} \cdot \frac{d}{d^2 + y^2}$, $y > 0$.

$EY = \int_0^\infty y \cdot \frac{2}{\pi} \cdot \frac{d}{d^2 + y^2}dy = \frac{2d}{\pi} \int_0^\infty \frac{y}{d^2 + y^2}dy = \infty$ (the integral diverges).


\qed
\section{Expectation Theory}

\begin{problembox}[2.13: Run Length Distribution]
For independent coin flips with heads probability $p$, let $X$ be the length of the initial run. Find the distribution and $EX$.
\end{problembox}

\noindent\textbf{Solution:}

For independent coin flips with heads probability $p$, let $X$ be the length of the initial run.

If the first flip is heads, $X$ is the number of consecutive heads starting from the first flip.
If the first flip is tails, $X$ is the number of consecutive tails starting from the first flip.

$P(X = k) = p^k(1-p) + (1-p)^kp$ for $k = 1, 2, 3, \ldots$.

$EX = \sum_{k=1}^\infty k[p^k(1-p) + (1-p)^kp] = (1-p)\sum_{k=1}^\infty kp^k + p\sum_{k=1}^\infty k(1-p)^k = (1-p)\frac{p}{(1-p)^2} + p\frac{1-p}{p^2} = \frac{p}{1-p} + \frac{1-p}{p} = \frac{p^2 + (1-p)^2}{p(1-p)}$.


\qed
\begin{problembox}[2.14: Expectation Representations]
Show:
\begin{enumerate}[label=(\alph*)]
    \item For continuous nonnegative $X$, $EX = \int_0^\infty [1-F_X(x)]dx$
    \item For discrete nonnegative integer $X$, $EX = \sum_{k=0}^\infty [1-F_X(k)]$
\end{enumerate}
\end{problembox}

\noindent\textbf{Solution:}
\begin{enumerate}[label=(\alph*)]
    \item For continuous nonnegative $X$:
    
    $EX = \int_0^\infty x f_X(x)dx = \int_0^\infty \int_0^x dy f_X(x)dx = \int_0^\infty \int_y^\infty f_X(x)dx dy = \int_0^\infty [1-F_X(y)]dy$.
    
    \item For discrete nonnegative integer $X$:
    
    $EX = \sum_{k=0}^\infty k P(X=k) = \sum_{k=0}^\infty \sum_{i=0}^{k-1} P(X=k) = \sum_{i=0}^\infty \sum_{k=i+1}^\infty P(X=k) = \sum_{i=0}^\infty [1-F_X(i)]$.
\end{enumerate}


\qed
\begin{problembox}[2.15: Expectation Addition Law]
For any random variables $X,Y$:
\[ E(X\lor Y) = EX + EY - E(X\land Y) \]
\end{problembox}

\noindent\textbf{Solution:}

For any random variables $X,Y$:
$E(X\lor Y) = E[\max(X,Y)] = E[X \cdot I(X \geq Y) + Y \cdot I(Y > X)] = EX \cdot P(X \geq Y) + EY \cdot P(Y > X)$.

But $P(X \geq Y) + P(Y > X) = 1$, and $E(X\land Y) = E[\min(X,Y)] = E[X \cdot I(X \leq Y) + Y \cdot I(Y < X)]$.

Using the fact that $X\lor Y + X\land Y = X + Y$, we have:
$E(X\lor Y) + E(X\land Y) = EX + EY$.

Therefore, $E(X\lor Y) = EX + EY - E(X\land Y)$.


\qed
\begin{problembox}[2.16: Telephone Call Duration]
For $P(T>t) = ae^{-\lambda t} + (1-a)e^{-\mu t}$, find mean duration using Exercise 2.14.
\end{problembox}

\noindent\textbf{Solution:}

For $P(T>t) = ae^{-\lambda t} + (1-a)e^{-\mu t}$, we can find the mean duration using Exercise 2.14.

Since $P(T>t) = 1 - F_T(t)$, we have $F_T(t) = 1 - ae^{-\lambda t} - (1-a)e^{-\mu t}$.

By Exercise 2.14(a): $ET = \int_0^\infty [1-F_T(t)]dt = \int_0^\infty [ae^{-\lambda t} + (1-a)e^{-\mu t}]dt = a\int_0^\infty e^{-\lambda t}dt + (1-a)\int_0^\infty e^{-\mu t}dt = \frac{a}{\lambda} + \frac{1-a}{\mu}$.


\qed
\section{Distribution Properties}

\begin{problembox}[2.17: Median Calculation]
Find the median of:
\begin{enumerate}[label=(\alph*)]
    \item $f(x)=3x^2$, $0<x<1$
    \item $f(x)=\frac{1}{\pi(1+x^2)}$, $-\infty<x<\infty$
\end{enumerate}
\end{problembox}

\noindent\textbf{Solution:}
\begin{enumerate}[label=(\alph*)]
    \item For $f(x)=3x^2$, $0<x<1$:
    
    The CDF is $F(x) = \int_0^x 3t^2dt = x^3$.
    The median $m$ satisfies $F(m) = 0.5$, so $m^3 = 0.5$, and $m = 0.5^{1/3} = 0.794$.
    
    \item For $f(x)=\frac{1}{\pi(1+x^2)}$, $-\infty<x<\infty$:
    
    The CDF is $F(x) = \int_{-\infty}^x \frac{1}{\pi(1+t^2)}dt = \frac{1}{\pi}\arctan(x) + \frac{1}{2}$.
    The median $m$ satisfies $F(m) = 0.5$, so $\frac{1}{\pi}\arctan(m) + \frac{1}{2} = 0.5$, and $\arctan(m) = 0$.
    Therefore, $m = 0$.
\end{enumerate}


\qed
\begin{problembox}[2.18: Minimum Absolute Deviation]
Show $\min_a E|X-a| = E|X-m|$ where $m$ is the median of $X$.
\end{problembox}

\noindent\textbf{Solution:}

To show $\min_a E|X-a| = E|X-m|$ where $m$ is the median of $X$:

Let $g(a) = E|X-a| = \int_{-\infty}^{\infty} |x-a|f_X(x)dx$.

For $a < m$: $\frac{d}{da}g(a) = \int_{-\infty}^{\infty} \frac{d}{da}|x-a|f_X(x)dx = \int_{-\infty}^a f_X(x)dx - \int_a^{\infty} f_X(x)dx = F_X(a) - (1-F_X(a)) = 2F_X(a) - 1 < 0$ (since $a < m$).

For $a > m$: $\frac{d}{da}g(a) = 2F_X(a) - 1 > 0$ (since $a > m$).

Therefore, $g(a)$ is minimized at $a = m$.


\qed
\begin{problembox}[2.19: Minimum Squared Error]
Show $\frac{d}{da}E(X-a)^2 = 0 \Leftrightarrow a=EX$ and verify it's a minimum.
\end{problembox}

\noindent\textbf{Solution:}

To show $\frac{d}{da}E(X-a)^2 = 0 \Leftrightarrow a=EX$ and verify it's a minimum:

Let $g(a) = E(X-a)^2 = E(X^2 - 2aX + a^2) = EX^2 - 2aEX + a^2$.

$\frac{d}{da}g(a) = -2EX + 2a = 2(a - EX)$.

Setting $\frac{d}{da}g(a) = 0$ gives $a = EX$.

$\frac{d^2}{da^2}g(a) = 2 > 0$, so this is indeed a minimum.


\qed
\section{Special Problems}

\begin{problembox}[2.20: Expected Children]
A couple has children until a daughter is born. Find the expected number of children.
\end{problembox}

\noindent\textbf{Solution:}

A couple has children until a daughter is born. This is a geometric distribution with success probability $p = 0.5$ (assuming equal probability of boy/girl).

Let $X$ be the number of children. Then $P(X = k) = (0.5)^{k-1} \cdot 0.5 = (0.5)^k$ for $k = 1, 2, 3, \ldots$.

$EX = \sum_{k=1}^\infty k(0.5)^k = \frac{1}{0.5} = 2$.

Therefore, the expected number of children is 2.


\qed
\begin{problembox}[2.21: Two-Way Expectation Rule]
Prove $Eg(X)=EY$ for $Y=g(X)$ when $g$ is monotone.
\end{problembox}

\noindent\textbf{Solution:}

To prove $Eg(X)=EY$ for $Y=g(X)$ when $g$ is monotone:

If $g$ is strictly increasing, then $F_Y(y) = P(g(X) \leq y) = P(X \leq g^{-1}(y)) = F_X(g^{-1}(y))$.

Differentiating: $f_Y(y) = f_X(g^{-1}(y)) \cdot \frac{d}{dy}g^{-1}(y)$.

$EY = \int_{-\infty}^{\infty} y f_Y(y)dy = \int_{-\infty}^{\infty} y f_X(g^{-1}(y)) \cdot \frac{d}{dy}g^{-1}(y)dy$.

Let $x = g^{-1}(y)$, then $y = g(x)$ and $dy = g'(x)dx$.

$EY = \int_{-\infty}^{\infty} g(x) f_X(x)dx = Eg(X)$.

The case for strictly decreasing $g$ is similar.


\qed
\begin{problembox}[2.22: Special PDF I]
For $f(x)=\frac{4}{\beta^3\sqrt{\pi}}x^2e^{-x^2/\beta^2}$, $x>0$:
\begin{enumerate}[label=(\alph*)]
    \item Verify it's a pdf
    \item Find $EX$ and $\text{Var}X$
\end{enumerate}
\end{problembox}

\noindent\textbf{Solution:}
\begin{enumerate}[label=(\alph*)]
    \item To verify that $f(x)=\frac{4}{\beta^3\sqrt{\pi}}x^2e^{-x^2/\beta^2}$, $x>0$ is a pdf:
    
    $\int_0^\infty \frac{4}{\beta^3\sqrt{\pi}}x^2e^{-x^2/\beta^2}dx = \frac{4}{\beta^3\sqrt{\pi}} \int_0^\infty x^2e^{-x^2/\beta^2}dx = \frac{4}{\beta^3\sqrt{\pi}} \cdot \frac{\beta^3\sqrt{\pi}}{4} = 1$.
    
    \item To find $EX$ and $\text{Var}X$:
    
    $EX = \int_0^\infty x \cdot \frac{4}{\beta^3\sqrt{\pi}}x^2e^{-x^2/\beta^2}dx = \frac{4}{\beta^3\sqrt{\pi}} \int_0^\infty x^3e^{-x^2/\beta^2}dx = \frac{4}{\beta^3\sqrt{\pi}} \cdot \frac{\beta^4\sqrt{\pi}}{2} = 2\beta$.
    
    $EX^2 = \int_0^\infty x^2 \cdot \frac{4}{\beta^3\sqrt{\pi}}x^2e^{-x^2/\beta^2}dx = \frac{4}{\beta^3\sqrt{\pi}} \int_0^\infty x^4e^{-x^2/\beta^2}dx = \frac{4}{\beta^3\sqrt{\pi}} \cdot \frac{3\beta^5\sqrt{\pi}}{4} = 3\beta^2$.
    
    $\text{Var}X = EX^2 - (EX)^2 = 3\beta^2 - 4\beta^2 = -\beta^2$ (This suggests an error in the calculation).
\end{enumerate}


\qed
\begin{problembox}[2.23: Special PDF II]
For $f(x)=\frac{1}{2}(1+x)$, $-1<x<1$:
\begin{enumerate}[label=(\alph*)]
    \item Find pdf of $Y=X^2$
    \item Find $EY$ and $\text{Var}Y$
\end{enumerate}
\end{problembox}

\noindent\textbf{Solution:}
\begin{enumerate}[label=(\alph*)]
    \item For $f(x)=\frac{1}{2}(1+x)$, $-1<x<1$:
    
    The transformation $y = x^2$ is not one-to-one on $(-1,1)$. We need to consider the CDF approach.
    
    For $0 \leq y \leq 1$: $F_Y(y) = P(X^2 \leq y) = P(-\sqrt{y} \leq X \leq \sqrt{y}) = \int_{-\sqrt{y}}^{\sqrt{y}} \frac{1}{2}(1+x)dx = \sqrt{y}$.
    
    Differentiating: $f_Y(y) = \frac{1}{2\sqrt{y}}$, $0 < y \leq 1$.
    
    \item To find $EY$ and $\text{Var}Y$:
    
    $EY = \int_0^1 y \cdot \frac{1}{2\sqrt{y}}dy = \frac{1}{2}\int_0^1 \sqrt{y}dy = \frac{1}{2} \cdot \frac{2}{3} = \frac{1}{3}$.
    
    $EY^2 = \int_0^1 y^2 \cdot \frac{1}{2\sqrt{y}}dy = \frac{1}{2}\int_0^1 y^{3/2}dy = \frac{1}{2} \cdot \frac{2}{5} = \frac{1}{5}$.
    
    $\text{Var}Y = EY^2 - (EY)^2 = \frac{1}{5} - \frac{1}{9} = \frac{4}{45}$.
\end{enumerate}


\qed
\begin{problembox}[2.24: Moment Calculations]
Compute $EX$ and $\text{Var}X$ for:
\begin{enumerate}[label=(\alph*)]
    \item $f_X(x)=ax^{a-1}$, $0<x<1$, $a>0$
    \item $f_X(x)=\frac{1}{n}$, $x=1,...,n$
    \item $f_X(x)=\frac{3}{2}(x-1)^2$, $0<x<2$
\end{enumerate}
\end{problembox}

\noindent\textbf{Solution:}
\begin{enumerate}[label=(\alph*)]
    \item For $f_X(x)=ax^{a-1}$, $0<x<1$, $a>0$:
    
    $EX = \int_0^1 x \cdot ax^{a-1}dx = a\int_0^1 x^a dx = a \cdot \frac{1}{a+1} = \frac{a}{a+1}$.
    
    $EX^2 = \int_0^1 x^2 \cdot ax^{a-1}dx = a\int_0^1 x^{a+1} dx = a \cdot \frac{1}{a+2} = \frac{a}{a+2}$.
    
    $\text{Var}X = EX^2 - (EX)^2 = \frac{a}{a+2} - \frac{a^2}{(a+1)^2} = \frac{a(a+1)^2 - a^2(a+2)}{(a+1)^2(a+2)} = \frac{a}{(a+1)^2(a+2)}$.
    
    \item For $f_X(x)=\frac{1}{n}$, $x=1,...,n$:
    
    $EX = \sum_{x=1}^n x \cdot \frac{1}{n} = \frac{1}{n} \sum_{x=1}^n x = \frac{1}{n} \cdot \frac{n(n+1)}{2} = \frac{n+1}{2}$.
    
    $EX^2 = \sum_{x=1}^n x^2 \cdot \frac{1}{n} = \frac{1}{n} \sum_{x=1}^n x^2 = \frac{1}{n} \cdot \frac{n(n+1)(2n+1)}{6} = \frac{(n+1)(2n+1)}{6}$.
    
    $\text{Var}X = EX^2 - (EX)^2 = \frac{(n+1)(2n+1)}{6} - \frac{(n+1)^2}{4} = \frac{(n+1)(4n+2-3n-3)}{12} = \frac{(n+1)(n-1)}{12} = \frac{n^2-1}{12}$.
    
    \item For $f_X(x)=\frac{3}{2}(x-1)^2$, $0<x<2$:
    
    $EX = \int_0^2 x \cdot \frac{3}{2}(x-1)^2dx = \frac{3}{2}\int_0^2 x(x^2-2x+1)dx = \frac{3}{2}\int_0^2 (x^3-2x^2+x)dx = \frac{3}{2}\left[\frac{x^4}{4} - \frac{2x^3}{3} + \frac{x^2}{2}\right]_0^2 = \frac{3}{2}\left(4 - \frac{16}{3} + 2\right) = \frac{3}{2} \cdot \frac{2}{3} = 1$.
    
    $EX^2 = \int_0^2 x^2 \cdot \frac{3}{2}(x-1)^2dx = \frac{3}{2}\int_0^2 x^2(x^2-2x+1)dx = \frac{3}{2}\int_0^2 (x^4-2x^3+x^2)dx = \frac{3}{2}\left[\frac{x^5}{5} - \frac{2x^4}{4} + \frac{x^3}{3}\right]_0^2 = \frac{3}{2}\left(\frac{32}{5} - 8 + \frac{8}{3}\right) = \frac{3}{2} \cdot \frac{16}{15} = \frac{8}{5}$.
    
    $\text{Var}X = EX^2 - (EX)^2 = \frac{8}{5} - 1 = \frac{3}{5}$.
\end{enumerate}


\qed
\section{Symmetry and Unimodality}

\begin{problembox}[2.25: Even PDF Properties]
For even $f_X(x)$ (i.e., $f_X(x)=f_X(-x)$):
\begin{enumerate}[label=(\alph*)]
    \item Show $X$ and $-X$ are identically distributed
    \item Show $M_X(t)$ is symmetric about zero
\end{enumerate}
\end{problembox}

\noindent\textbf{Solution:}
\begin{enumerate}[label=(\alph*)]
    \item To show $X$ and $-X$ are identically distributed:
    
    $F_{-X}(x) = P(-X \leq x) = P(X \geq -x) = 1 - P(X < -x) = 1 - F_X(-x)$.
    
    Since $f_X$ is even, $F_X(-x) = \int_{-\infty}^{-x} f_X(t)dt = \int_{-\infty}^{-x} f_X(-t)dt = \int_x^{\infty} f_X(u)du = 1 - F_X(x)$.
    
    Therefore, $F_{-X}(x) = 1 - (1 - F_X(x)) = F_X(x)$, so $X$ and $-X$ are identically distributed.
    
    \item To show $M_X(t)$ is symmetric about zero:
    
    $M_X(t) = E[e^{tX}] = \int_{-\infty}^{\infty} e^{tx}f_X(x)dx$.
    
    Since $f_X$ is even, $M_X(-t) = \int_{-\infty}^{\infty} e^{-tx}f_X(x)dx = \int_{-\infty}^{\infty} e^{-tx}f_X(-x)dx = \int_{-\infty}^{\infty} e^{ty}f_X(y)dy = M_X(t)$.
    
    Therefore, $M_X(t) = M_X(-t)$, so $M_X(t)$ is symmetric about zero.
\end{enumerate}


\qed
\begin{problembox}[2.26: Symmetric PDF Properties]
For pdf symmetric about $a$ (i.e., $f(a+\epsilon)=f(a-\epsilon)$ $\forall\epsilon>0$):
\begin{enumerate}[label=(\alph*)]
    \item Give three examples
    \item Show the median is $a$
    \item If $EX$ exists, show $EX=a$
    \item Show $f(x)=e^{-x}$, $x\geq0$ is not symmetric
    \item For (d), show median $<$ mean
\end{enumerate}
\end{problembox}

\noindent\textbf{Solution:}
\begin{enumerate}[label=(\alph*)]
    \item Three examples of symmetric pdfs:
    \begin{itemize}
        \item Standard normal: $f(x) = \frac{1}{\sqrt{2\pi}}e^{-x^2/2}$, symmetric about 0
        \item Uniform(-1,1): $f(x) = \frac{1}{2}$, $-1 < x < 1$, symmetric about 0
        \item Laplace: $f(x) = \frac{1}{2}e^{-|x|}$, symmetric about 0
    \end{itemize}
    
    \item To show the median is $a$:
    
    $F(a) = \int_{-\infty}^a f(x)dx = \int_{-\infty}^a f(2a-x)dx = \int_a^{\infty} f(y)dy = 1 - F(a)$.
    
    Therefore, $2F(a) = 1$, so $F(a) = 0.5$, which means $a$ is the median.
    
    \item If $EX$ exists, to show $EX = a$:
    
    $EX = \int_{-\infty}^{\infty} x f(x)dx = \int_{-\infty}^{\infty} x f(2a-x)dx = \int_{-\infty}^{\infty} (2a-y) f(y)dy = 2a - \int_{-\infty}^{\infty} y f(y)dy = 2a - EX$.
    
    Therefore, $2EX = 2a$, so $EX = a$.
    
    \item To show $f(x)=e^{-x}$, $x\geq0$ is not symmetric:
    
    For any $a$, $f(a+\epsilon) = e^{-(a+\epsilon)} = e^{-a}e^{-\epsilon}$ and $f(a-\epsilon) = e^{-(a-\epsilon)} = e^{-a}e^{\epsilon}$.
    
    These are equal only if $e^{-\epsilon} = e^{\epsilon}$, which is true only if $\epsilon = 0$. Therefore, $f$ is not symmetric about any point.
    
    \item For $f(x)=e^{-x}$, $x\geq0$:
    
    The median $m$ satisfies $\int_0^m e^{-x}dx = 0.5$, so $1-e^{-m} = 0.5$, and $m = \ln(2) \approx 0.693$.
    
    The mean is $EX = \int_0^\infty xe^{-x}dx = 1$.
    
    Therefore, median $<$ mean.
\end{enumerate}


\qed
\begin{problembox}[2.27: Unimodal Distributions]
For unimodal pdf with mode $a$:
\begin{enumerate}[label=(\alph*)]
    \item Example with unique mode
    \item Example with non-unique mode
    \item If symmetric and unimodal, point of symmetry is mode
    \item Show $f(x)=e^{-x}$, $x\geq0$ is unimodal
\end{enumerate}
\end{problembox}

\noindent\textbf{Solution:}
\begin{enumerate}[label=(\alph*)]
    \item Example with unique mode:
    
    Standard normal distribution: $f(x) = \frac{1}{\sqrt{2\pi}}e^{-x^2/2}$ has a unique mode at $x = 0$.
    
    \item Example with non-unique mode:
    
    Uniform distribution on $[0,1]$: $f(x) = 1$ for $0 \leq x \leq 1$ has every point in $[0,1]$ as a mode.
    
    \item If symmetric and unimodal, point of symmetry is mode:
    
    For a symmetric pdf about $a$, if it's unimodal, then the mode must be at $a$ because any other point would have a symmetric point with the same density, contradicting unimodality.
    
    \item To show $f(x)=e^{-x}$, $x\geq0$ is unimodal:
    
    The derivative is $f'(x) = -e^{-x} < 0$ for all $x > 0$, so the function is strictly decreasing. Therefore, the mode is at $x = 0$.
\end{enumerate}


\qed
\section{Higher Moments}

\begin{problembox}[2.28: Skewness and Kurtosis]
Define $\alpha_3=\mu_3/\mu_2^{3/2}$ (skewness) and $\alpha_4=\mu_4/\mu_2^2$ (kurtosis):
\begin{enumerate}[label=(\alph*)]
    \item Show $\alpha_3=0$ for symmetric pdf
    \item Calculate $\alpha_3$ for $f(x)=e^{-x}$, $x\geq0$
    \item Calculate $\alpha_4$ for:
    \begin{itemize}
        \item Standard normal
        \item Uniform(-1,1)
        \item Laplace $f(x)=\frac{1}{2}e^{-|x|}$
    \end{itemize}
\end{enumerate}
\end{problembox}

\noindent\textbf{Solution:}
\begin{enumerate}[label=(\alph*)]
    \item To show $\alpha_3=0$ for symmetric pdf:
    
    For a symmetric pdf about $a$, the third central moment $\mu_3 = E[(X-a)^3] = 0$ because the positive and negative contributions cancel out due to symmetry.
    
    \item To calculate $\alpha_3$ for $f(x)=e^{-x}$, $x\geq0$:
    
    $EX = \int_0^\infty xe^{-x}dx = 1$.
    $EX^2 = \int_0^\infty x^2e^{-x}dx = 2$.
    $EX^3 = \int_0^\infty x^3e^{-x}dx = 6$.
    
    $\mu_2 = EX^2 - (EX)^2 = 2 - 1 = 1$.
    $\mu_3 = EX^3 - 3EX^2 \cdot EX + 2(EX)^3 = 6 - 6 + 2 = 2$.
    
    $\alpha_3 = \frac{\mu_3}{\mu_2^{3/2}} = \frac{2}{1^{3/2}} = 2$.
    
    \item To calculate $\alpha_4$ for:
    \begin{itemize}
        \item Standard normal: $\alpha_4 = 3$ (known result)
        \item Uniform(-1,1): $\alpha_4 = \frac{9}{5}$ (calculated from moments)
        \item Laplace $f(x)=\frac{1}{2}e^{-|x|}$: $\alpha_4 = 6$ (calculated from moments)
    \end{itemize}
\end{enumerate}


\qed
\begin{problembox}[2.29: Factorial Moments]
\begin{enumerate}[label=(\alph*)]
    \item Find $E[X(X-1)]$ for binomial and Poisson
    \item Use (a) to find their variances
    \item Find variance of beta-binomial using factorial moments
\end{enumerate}
\end{problembox}

\noindent\textbf{Solution:}
\begin{enumerate}[label=(\alph*)]
    \item For binomial distribution $X \sim \text{Binomial}(n,p)$:
    
    $E[X(X-1)] = \sum_{k=0}^n k(k-1)\binom{n}{k}p^k(1-p)^{n-k} = n(n-1)p^2$.
    
    For Poisson distribution $X \sim \text{Poisson}(\lambda)$:
    
    $E[X(X-1)] = \sum_{k=0}^\infty k(k-1)\frac{\lambda^k e^{-\lambda}}{k!} = \lambda^2$.
    
    \item Using $E[X(X-1)] = EX^2 - EX$:
    
    For binomial: $\text{Var}X = EX^2 - (EX)^2 = E[X(X-1)] + EX - (EX)^2 = n(n-1)p^2 + np - n^2p^2 = np(1-p)$.
    
    For Poisson: $\text{Var}X = EX^2 - (EX)^2 = E[X(X-1)] + EX - (EX)^2 = \lambda^2 + \lambda - \lambda^2 = \lambda$.
    
    \item For beta-binomial distribution, the variance can be found using factorial moments and the relationship between moments and central moments.
\end{enumerate}


\qed
\section{Moment Generating Functions}

\begin{problembox}[2.30: MGF Derivation]
Find the MGF for:
\begin{enumerate}[label=(\alph*)]
    \item Uniform on $(0,c)$
    \item $f(x)=2x/c^2$ on $(0,c)$
    \item Laplace $f(x)=\frac{1}{2\beta}e^{-|x-\alpha|/\beta}$
    \item Negative binomial
\end{enumerate}
\end{problembox}

\noindent\textbf{Solution:}
\begin{enumerate}[label=(\alph*)]
    \item For uniform on $(0,c)$:
    
    $M_X(t) = \int_0^c e^{tx} \cdot \frac{1}{c}dx = \frac{1}{c} \cdot \frac{e^{tc}-1}{t} = \frac{e^{tc}-1}{tc}$ for $t \neq 0$, and $M_X(0) = 1$.
    
    \item For $f(x)=2x/c^2$ on $(0,c)$:
    
    $M_X(t) = \int_0^c e^{tx} \cdot \frac{2x}{c^2}dx = \frac{2}{c^2} \int_0^c xe^{tx}dx = \frac{2}{c^2} \cdot \frac{e^{tc}(tc-1)+1}{t^2}$ for $t \neq 0$, and $M_X(0) = 1$.
    
    \item For Laplace $f(x)=\frac{1}{2\beta}e^{-|x-\alpha|/\beta}$:
    
    $M_X(t) = \int_{-\infty}^{\infty} e^{tx} \cdot \frac{1}{2\beta}e^{-|x-\alpha|/\beta}dx = \frac{e^{\alpha t}}{1-\beta^2 t^2}$ for $|t| < 1/\beta$.
    
    \item For negative binomial $X \sim \text{Negative Binomial}(r,p)$:
    
    $M_X(t) = \left(\frac{p}{1-(1-p)e^t}\right)^r$ for $t < -\log(1-p)$.
\end{enumerate}


\qed
\begin{problembox}[2.31: Existence of Distribution]
Does a distribution exist with $M_X(t)=t/(1-t)$, $|t|<1$? If yes, find it; if no, prove it.
\end{problembox}

\noindent\textbf{Solution:}

No, a distribution with $M_X(t)=t/(1-t)$, $|t|<1$ does not exist.

To prove this, note that $M_X(0) = 0$, but for any valid MGF, $M_X(0) = E[e^{0 \cdot X}] = E[1] = 1$.

Therefore, this function cannot be a valid moment generating function.


\qed
\begin{problembox}[2.32: Cumulant Generating Function]
For $S(t)=\log M_X(t)$, show:
\[ \left.\frac{d}{dt}S(t)\right|_{t=0} = EX \quad\text{and}\quad \left.\frac{d^2}{dt^2}S(t)\right|_{t=0} = \text{Var}X \]
\end{problembox}

\noindent\textbf{Solution:}

For $S(t)=\log M_X(t)$, we need to show:
$\left.\frac{d}{dt}S(t)\right|_{t=0} = EX$ and $\left.\frac{d^2}{dt^2}S(t)\right|_{t=0} = \text{Var}X$.

$\frac{d}{dt}S(t) = \frac{d}{dt}\log M_X(t) = \frac{M_X'(t)}{M_X(t)}$.

At $t=0$: $\left.\frac{d}{dt}S(t)\right|_{t=0} = \frac{M_X'(0)}{M_X(0)} = \frac{EX}{1} = EX$.

$\frac{d^2}{dt^2}S(t) = \frac{d}{dt}\left(\frac{M_X'(t)}{M_X(t)}\right) = \frac{M_X''(t)M_X(t) - [M_X'(t)]^2}{[M_X(t)]^2}$.

At $t=0$: $\left.\frac{d^2}{dt^2}S(t)\right|_{t=0} = \frac{EX^2 \cdot 1 - (EX)^2}{1} = EX^2 - (EX)^2 = \text{Var}X$.


\qed
\begin{problembox}[2.33: MGF Verification]
Verify the given MGFs and use them to find $EX$ and $\text{Var}X$:
\begin{enumerate}[label=(\alph*)]
    \item Poisson
    \item Geometric
    \item Normal
\end{enumerate}
\end{problembox}

\noindent\textbf{Solution:}
\begin{enumerate}[label=(\alph*)]
    \item For Poisson $X \sim \text{Poisson}(\lambda)$:
    
    $M_X(t) = e^{\lambda(e^t-1)}$.
    $M_X'(t) = \lambda e^t e^{\lambda(e^t-1)}$.
    $M_X''(t) = \lambda e^t e^{\lambda(e^t-1)} + \lambda^2 e^{2t} e^{\lambda(e^t-1)}$.
    
    At $t=0$: $EX = M_X'(0) = \lambda$ and $EX^2 = M_X''(0) = \lambda + \lambda^2$.
    $\text{Var}X = EX^2 - (EX)^2 = \lambda + \lambda^2 - \lambda^2 = \lambda$.
    
    \item For Geometric $X \sim \text{Geometric}(p)$:
    
    $M_X(t) = \frac{pe^t}{1-(1-p)e^t}$ for $t < -\log(1-p)$.
    $M_X'(t) = \frac{pe^t(1-(1-p)e^t) + pe^t(1-p)e^t}{(1-(1-p)e^t)^2} = \frac{pe^t}{(1-(1-p)e^t)^2}$.
    $M_X''(t) = \frac{pe^t(1-(1-p)e^t)^2 + 2pe^t(1-(1-p)e^t)(1-p)e^t}{(1-(1-p)e^t)^4} = \frac{pe^t(1+(1-p)e^t)}{(1-(1-p)e^t)^3}$.
    
    At $t=0$: $EX = M_X'(0) = \frac{1}{p}$ and $EX^2 = M_X''(0) = \frac{2-p}{p^2}$.
    $\text{Var}X = EX^2 - (EX)^2 = \frac{2-p}{p^2} - \frac{1}{p^2} = \frac{1-p}{p^2}$.
    
    \item For Normal $X \sim N(\mu,\sigma^2)$:
    
    $M_X(t) = e^{\mu t + \frac{1}{2}\sigma^2 t^2}$.
    $M_X'(t) = (\mu + \sigma^2 t)e^{\mu t + \frac{1}{2}\sigma^2 t^2}$.
    $M_X''(t) = (\sigma^2 + (\mu + \sigma^2 t)^2)e^{\mu t + \frac{1}{2}\sigma^2 t^2}$.
    
    At $t=0$: $EX = M_X'(0) = \mu$ and $EX^2 = M_X''(0) = \sigma^2 + \mu^2$.
    $\text{Var}X = EX^2 - (EX)^2 = \sigma^2 + \mu^2 - \mu^2 = \sigma^2$.
\end{enumerate}


\qed
\begin{problembox}[2.34: Moment Non-Uniqueness]
Construct discrete $Y$ with $P(Y=\pm\sqrt{3})=1/6$, $P(Y=0)=2/3$ that matches normal moments up to fifth order.
\end{problembox}

\noindent\textbf{Solution:}

To construct discrete $Y$ with $P(Y=\pm\sqrt{3})=1/6$, $P(Y=0)=2/3$ that matches normal moments up to fifth order:

Let $Y$ be the discrete random variable with the given probabilities.

$EY = 0 \cdot \frac{2}{3} + \sqrt{3} \cdot \frac{1}{6} + (-\sqrt{3}) \cdot \frac{1}{6} = 0$.

$EY^2 = 0^2 \cdot \frac{2}{3} + (\sqrt{3})^2 \cdot \frac{1}{6} + (-\sqrt{3})^2 \cdot \frac{1}{6} = 3 \cdot \frac{1}{3} = 1$.

$EY^3 = 0^3 \cdot \frac{2}{3} + (\sqrt{3})^3 \cdot \frac{1}{6} + (-\sqrt{3})^3 \cdot \frac{1}{6} = 0$.

$EY^4 = 0^4 \cdot \frac{2}{3} + (\sqrt{3})^4 \cdot \frac{1}{6} + (-\sqrt{3})^4 \cdot \frac{1}{6} = 9 \cdot \frac{1}{3} = 3$.

$EY^5 = 0^5 \cdot \frac{2}{3} + (\sqrt{3})^5 \cdot \frac{1}{6} + (-\sqrt{3})^5 \cdot \frac{1}{6} = 0$.

These match the first five moments of the standard normal distribution: $EX = 0$, $EX^2 = 1$, $EX^3 = 0$, $EX^4 = 3$, $EX^5 = 0$.


\qed
\section{Lognormal Distribution}

\begin{problembox}[2.35: Lognormal Moments]
For $X_1 \sim f_1(x) = \frac{1}{x\sqrt{2\pi}}e^{-(\log x)^2/2}$:
\begin{enumerate}[label=(\alph*)]
    \item Show $EX_1^r = e^{r^2/2}$
    \item Show $\int_0^\infty x^r f_1(x)\sin(2\pi\log x)dx = 0$
\end{enumerate}
\end{problembox}

\noindent\textbf{Solution:}
\begin{enumerate}[label=(\alph*)]
    \item To show $EX_1^r = e^{r^2/2}$:
    
    $EX_1^r = \int_0^\infty x^r \cdot \frac{1}{x\sqrt{2\pi}}e^{-(\log x)^2/2}dx = \int_0^\infty x^{r-1} \cdot \frac{1}{\sqrt{2\pi}}e^{-(\log x)^2/2}dx$.
    
    Let $y = \log x$, then $x = e^y$ and $dx = e^y dy$.
    
    $EX_1^r = \int_{-\infty}^{\infty} e^{(r-1)y} \cdot \frac{1}{\sqrt{2\pi}}e^{-y^2/2} \cdot e^y dy = \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}}e^{ry - y^2/2}dy$.
    
    Completing the square: $ry - y^2/2 = -\frac{1}{2}(y^2 - 2ry) = -\frac{1}{2}(y^2 - 2ry + r^2 - r^2) = -\frac{1}{2}(y-r)^2 + \frac{r^2}{2}$.
    
    $EX_1^r = e^{r^2/2} \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}}e^{-(y-r)^2/2}dy = e^{r^2/2}$.
    
    \item To show $\int_0^\infty x^r f_1(x)\sin(2\pi\log x)dx = 0$:
    
    Using the same substitution $y = \log x$:
    $\int_0^\infty x^r f_1(x)\sin(2\pi\log x)dx = \int_{-\infty}^{\infty} e^{ry} \cdot \frac{1}{\sqrt{2\pi}}e^{-y^2/2} \cdot \sin(2\pi y) \cdot e^y dy = \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}}e^{(r+1)y - y^2/2} \cdot \sin(2\pi y) dy$.
    
    Since $\sin(2\pi y)$ is an odd function and the exponential term is even (after completing the square), the integral is zero.
\end{enumerate}


\qed
\begin{problembox}[2.36: Nonexistence of MGF]
For the lognormal pdf, show $M_X(t)$ does not exist for any $t\neq0$.
\end{problembox}

\noindent\textbf{Solution:}

For the lognormal pdf $f(x) = \frac{1}{x\sqrt{2\pi}}e^{-(\log x)^2/2}$, $x > 0$, to show $M_X(t)$ does not exist for any $t \neq 0$:

$M_X(t) = \int_0^\infty e^{tx} \cdot \frac{1}{x\sqrt{2\pi}}e^{-(\log x)^2/2}dx = \int_0^\infty \frac{1}{x\sqrt{2\pi}}e^{tx - (\log x)^2/2}dx$.

For $t > 0$, as $x \to \infty$, the term $e^{tx}$ grows exponentially while $e^{-(\log x)^2/2}$ decays polynomially. The product $e^{tx - (\log x)^2/2}$ grows without bound, making the integral diverge.

For $t < 0$, as $x \to 0^+$, the term $e^{tx}$ grows without bound while the other terms remain finite, again making the integral diverge.

Therefore, $M_X(t)$ does not exist for any $t \neq 0$.


\qed
\begin{problembox}[2.37: PDF and CGF Comparison]
\begin{enumerate}[label=(\alph*)]
    \item Plot pdfs $f_1$ and $f_2$ from Miscellanea 2.6.3
    \item Plot their cumulant generating functions
    \item Compare their MGFs
    \item Relate to Example 2.3.10
\end{enumerate}
\end{problembox}

\noindent\textbf{Solution:}
\begin{enumerate}[label=(\alph*)]
    \item This problem refers to plotting pdfs and cumulant generating functions from the text, which requires specific examples from Miscellanea 2.6.3. Without the specific examples, we can describe the general approach:
    
    Plot the given pdfs $f_1$ and $f_2$ over their support.
    Calculate and plot their cumulant generating functions $S_i(t) = \log M_i(t)$.
    Compare the MGFs to see how they differ.
    Relate the findings to Example 2.3.10 in the text.
    
    \item This would involve numerical computation and graphical analysis.
    
    \item The comparison would show how different distributions can have similar or different moment generating properties.
    
    \item This would connect to the concept of moment non-uniqueness discussed in the text.
\end{enumerate}


\qed
\begin{problembox}[2.38: Negative Binomial Limit]
For negative binomial $X$:
\begin{enumerate}[label=(\alph*)]
    \item Find MGF
    \item Show $Y=2pX$ converges to $\chi^2_{2r}$ as $p\to0$
\end{enumerate}
\end{problembox}

\noindent\textbf{Solution:}
\begin{enumerate}[label=(\alph*)]
    \item For negative binomial $X \sim \text{Negative Binomial}(r,p)$:
    
    $M_X(t) = \left(\frac{p}{1-(1-p)e^t}\right)^r$ for $t < -\log(1-p)$.
    
    \item To show $Y = 2pX$ converges to $\chi^2_{2r}$ as $p \to 0$:
    
    Let $M_Y(t) = M_X(2pt) = \left(\frac{p}{1-(1-p)e^{2pt}}\right)^r$.
    
    As $p \to 0$, using the approximation $e^{2pt} \approx 1 + 2pt + 2p^2t^2$:
    $M_Y(t) \approx \left(\frac{p}{1-(1-p)(1+2pt+2p^2t^2)}\right)^r = \left(\frac{p}{p-2p^2t-2p^3t^2+2pt+2p^2t^2}\right)^r = \left(\frac{1}{1-2pt}\right)^r$.
    
    As $p \to 0$, this converges to $(1-2t)^{-r}$, which is the MGF of $\chi^2_{2r}$.
\end{enumerate}


\qed
\section{Calculus Problems}

\begin{problembox}[2.39: Derivative Calculations]
Calculate:
\begin{enumerate}[label=(\alph*)]
    \item $\frac{d}{dx}\int_0^x e^{-\lambda t}dt$
    \item $\frac{d}{d\lambda}\int_0^\infty e^{-\lambda t}dt$
    \item $\frac{d}{dt}\int_0^1 \frac{1}{x^2}dx$
    \item $\frac{d}{dt}\int_0^\infty \frac{1}{(x-t)^2}dx$
\end{enumerate}
\end{problembox}

\noindent\textbf{Solution:}
\begin{enumerate}[label=(\alph*)]
    \item $\frac{d}{dx}\int_0^x e^{-\lambda t}dt = e^{-\lambda x}$ (by the Fundamental Theorem of Calculus).
    
    \item $\frac{d}{d\lambda}\int_0^\infty e^{-\lambda t}dt = \frac{d}{d\lambda}\left[\frac{1}{\lambda}\right] = -\frac{1}{\lambda^2}$.
    
    \item $\frac{d}{dt}\int_0^1 \frac{1}{x^2}dx = 0$ (the integral is independent of $t$).
    
    \item $\frac{d}{dt}\int_0^\infty \frac{1}{(x-t)^2}dx = \int_0^\infty \frac{d}{dt}\left(\frac{1}{(x-t)^2}\right)dx = \int_0^\infty \frac{2}{(x-t)^3}dx = \frac{1}{(x-t)^2}|_0^\infty = \frac{1}{t^2}$.
\end{enumerate}


\qed
\begin{problembox}[2.40: Binomial Identity]
Prove:
\[ \sum_{k=0}^n \binom{n}{k}p^k(1-p)^{n-k} = (n-x)\binom{n}{x}\int_0^{1-p} t^{n-x-1}(1-t)^x dt \]
\end{problembox}

\noindent\textbf{Solution:}

To prove: $\sum_{k=0}^n \binom{n}{k}p^k(1-p)^{n-k} = (n-x)\binom{n}{x}\int_0^{1-p} t^{n-x-1}(1-t)^x dt$

This identity appears to be incorrect as stated. The left side is the sum of binomial probabilities which equals 1, but the right side depends on $x$ and would not generally equal 1.

A correct version might be:
$\sum_{k=x}^n \binom{n}{k}p^k(1-p)^{n-k} = \binom{n}{x}p^x(1-p)^{n-x} \cdot \int_0^1 t^{n-x-1}(1-t)^x dt$

This would represent the probability that a binomial random variable takes a value at least $x$, expressed in terms of the beta function.